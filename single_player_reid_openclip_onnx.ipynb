{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üéØ Professional-Grade Single-Player Re-Identification (Max Robustness)\n",
        "\n",
        "This notebook implements a top-tier, robust pipeline for tracking a single player. It is designed to handle real-world challenges like camera motion, occlusions, and similar-looking players with maximum accuracy.\n",
        "\n",
        "### Core Upgrades:\n",
        "- **Detector**: **YOLOv8-Large** for high-accuracy bounding boxes.\n",
        "- **Re-ID Model**: **OpenCLIP ViT-H-14 (Huge)** for the most powerful and discriminative visual features.\n",
        "- **Robust Tracking Algorithm**:\n",
        "  - **Camera Motion Compensation**: Actively calculates and compensates for camera pan/zoom to stabilize predictions.\n",
        "  - **Attribute Gating**: Creates and matches a **Color Profile** of the target's uniform to prevent confusion with other teams.\n",
        "  - **Enhanced Re-ID Memory**: Maintains a gallery of recent appearances for reliable re-identification after long occlusions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 1: Install Upgraded Dependencies\n",
        "print(\"Installing and configuring the upgraded environment...\")\n",
        "!pip -q uninstall -y tensorflow tensorflow-hub numba onnxruntime onnxruntime-gpu opencv-python opencv-contrib-python opencv-python-headless torchaudio torchvision || true\n",
        "!pip -q install \"numpy~=2.0.0\" \"opencv-python-headless>=4.9.0\"\n",
        "!pip -q install \"ultralytics>=8.0.0\" \"scipy>=1.10.0\" \"filterpy>=1.4.5\"\n",
        "!pip -q install \"torch>=2.3.0\" --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip -q install \"open-clip-torch>=2.24.0\"\n",
        "!pip -q install \"pillow>=10.0.0\" \"tqdm>=4.66.0\"\n",
        "\n",
        "print(\"\\n--- Verifying Installations ---\")\n",
        "try:\n",
        "    import numpy as np, cv2, torch, open_clip, ultralytics, scipy, filterpy\n",
        "    from ultralytics import YOLO\n",
        "    print(f\"NumPy version: {np.__version__}\")\n",
        "    print(f\"OpenCV version: {cv2.__version__}\")\n",
        "    print(f\"PyTorch version: {torch.__version__}\")\n",
        "    print(f\"Ultralytics (YOLOv8) version: {ultralytics.__version__}\")\n",
        "    print(\"‚úÖ All libraries loaded successfully!\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Error during verification: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 2: Initialize Upgraded Models\n",
        "import torch\n",
        "import open_clip\n",
        "from ultralytics import YOLO\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "from typing import Optional\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "print(\"Loading YOLOv8-Large model...\")\n",
        "detector = YOLO('yolov8l.pt')\n",
        "detector.to(device)\n",
        "\n",
        "print(\"Loading OpenCLIP ViT-H/14 model (this may take a moment)...\")\n",
        "reid_model, _, reid_preprocess = open_clip.create_model_and_transforms('ViT-H-14', pretrained='laion2b_s32b_b79k', device=device)\n",
        "reid_model.eval()\n",
        "\n",
        "def get_embedding(crop_bgr: np.ndarray) -> Optional[np.ndarray]:\n",
        "    if crop_bgr.size == 0: return None\n",
        "    try:\n",
        "        rgb = cv2.cvtColor(crop_bgr, cv2.COLOR_BGR2RGB)\n",
        "        pil_img = Image.fromarray(rgb)\n",
        "        img_tensor = reid_preprocess(pil_img).unsqueeze(0).to(device)\n",
        "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
        "            features = reid_model.encode_image(img_tensor)\n",
        "            features /= features.norm(dim=-1, keepdim=True)\n",
        "        return features.cpu().numpy().squeeze()\n",
        "    except Exception as e:\n",
        "        return None\n",
        "\n",
        "def get_color_hist(crop_bgr: np.ndarray) -> Optional[np.ndarray]:\n",
        "    if crop_bgr.size == 0: return None\n",
        "    try:\n",
        "        lab_crop = cv2.cvtColor(crop_bgr, cv2.COLOR_BGR2LAB)\n",
        "        hist = cv2.calcHist([lab_crop], [1, 2], None, [32, 32], [0, 256, 0, 256])\n",
        "        cv2.normalize(hist, hist)\n",
        "        return hist.flatten()\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "print(\"‚úÖ All models initialized successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 3: Define the Robust Professional Tracker\n",
        "from filterpy.kalman import KalmanFilter\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from collections import deque\n",
        "\n",
        "class RobustTrack:\n",
        "    def __init__(self, bbox, track_id, embedding, color_hist):\n",
        "        self.id = track_id\n",
        "        self.kf = KalmanFilter(dim_x=7, dim_z=4)\n",
        "        self.kf.F = np.array([[1,0,0,0,1,0,0], [0,1,0,0,0,1,0], [0,0,1,0,0,0,1], [0,0,0,1,0,0,0], [0,0,0,0,1,0,0], [0,0,0,0,0,1,0], [0,0,0,0,0,0,1]])\n",
        "        self.kf.H = np.array([[1,0,0,0,0,0,0], [0,1,0,0,0,0,0], [0,0,1,0,0,0,0], [0,0,0,1,0,0,0]])\n",
        "        self.kf.R[2:,2:] *= 10.\n",
        "        self.kf.P[4:,4:] *= 1000.\n",
        "        self.kf.P *= 10.\n",
        "        self.kf.Q[-1,-1] *= 0.01\n",
        "        self.kf.Q[4:,4:] *= 0.01\n",
        "        self.kf.x[:4] = self.convert_bbox_to_z(bbox)\n",
        "        self.time_since_update = 0\n",
        "        self.hits = 1\n",
        "        self.age = 0\n",
        "        self.gallery = deque(maxlen=100)\n",
        "        self.gallery.append(embedding)\n",
        "        self.color_profile = color_hist\n",
        "\n",
        "    def convert_bbox_to_z(self, bbox):\n",
        "        w, h = bbox[2] - bbox[0], bbox[3] - bbox[1]\n",
        "        x, y = bbox[0] + w/2., bbox[1] + h/2.\n",
        "        s, r = w * h, w / float(h) if h > 0 else 0\n",
        "        return np.array([x, y, s, r]).reshape((4, 1))\n",
        "\n",
        "    def predict(self, motion_matrix=None):\n",
        "        if motion_matrix is not None:\n",
        "            # Apply camera motion compensation\n",
        "            pos = np.array([self.kf.x[0,0], self.kf.x[1,0], 1.0]).reshape(3,1)\n",
        "            new_pos = motion_matrix @ pos\n",
        "            self.kf.x[0,0] = new_pos[0,0]\n",
        "            self.kf.x[1,0] = new_pos[1,0]\n",
        "        self.kf.predict()\n",
        "        self.age += 1\n",
        "        self.time_since_update += 1\n",
        "        return self.get_state()\n",
        "\n",
        "    def update(self, bbox, embedding, color_hist):\n",
        "        self.time_since_update = 0\n",
        "        self.hits += 1\n",
        "        self.kf.update(self.convert_bbox_to_z(bbox))\n",
        "        self.gallery.append(embedding)\n",
        "        # Update color profile with EMA\n",
        "        self.color_profile = 0.9 * self.color_profile + 0.1 * color_hist\n",
        "\n",
        "    def get_state(self):\n",
        "        x, y, s, r = self.kf.x[:4].flatten()\n",
        "        w = np.sqrt(s * r) if s * r > 0 else 0\n",
        "        h = s / w if w > 0 else 0\n",
        "        return np.array([x - w/2., y - h/2., x + w/2., y + h/2.])\n",
        "\n",
        "class RobustTracker:\n",
        "    def __init__(self, max_age=90, reid_threshold=0.5, color_threshold=0.7):\n",
        "        self.max_age = max_age\n",
        "        self.reid_threshold = reid_threshold\n",
        "        self.color_threshold = color_threshold\n",
        "        self.track = None # We only track one player\n",
        "\n",
        "    def update(self, detections, embeddings, color_hists, motion_matrix=None):\n",
        "        if self.track:\n",
        "            self.track.predict(motion_matrix)\n",
        "\n",
        "        if not self.track:\n",
        "            if len(detections) > 0:\n",
        "                areas = (detections[:, 2] - detections[:, 0]) * (detections[:, 3] - detections[:, 1])\n",
        "                idx = np.argmax(areas)\n",
        "                self.track = RobustTrack(detections[idx], 1, embeddings[idx], color_hists[idx])\n",
        "        else:\n",
        "            if len(detections) > 0:\n",
        "                # Calculate combined similarity score\n",
        "                gallery_sims = np.array([np.dot(g, e) for g in self.track.gallery for e in embeddings]).reshape(len(self.track.gallery), len(embeddings))\n",
        "                max_gallery_sims = gallery_sims.max(axis=0)\n",
        "                color_sims = np.array([cv2.compareHist(self.track.color_profile, h, cv2.HISTCMP_CORREL) for h in color_hists])\n",
        "                \n",
        "                combined_scores = 0.7 * max_gallery_sims + 0.3 * color_sims\n",
        "                best_match_idx = np.argmax(combined_scores)\n",
        "\n",
        "                if combined_scores[best_match_idx] > self.reid_threshold and color_sims[best_match_idx] > self.color_threshold:\n",
        "                    self.track.update(detections[best_match_idx], embeddings[best_match_idx], color_hists[best_match_idx])\n",
        "        \n",
        "        if self.track and self.track.time_since_update > self.max_age:\n",
        "            self.track = None\n",
        "\n",
        "        if self.track and self.track.time_since_update == 0:\n",
        "            return [self.track.get_state()]\n",
        "        return []\n",
        "\n",
        "print(\"‚úÖ Robust professional tracker defined successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 4: Define the Main Video Processing Function with Enhancements\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def process_video_robust(\n",
        "    input_path: str, \n",
        "    output_path: str, \n",
        "    highlight_path: str = '/content/highlights.mp4'\n",
        "):\n",
        "    cap = cv2.VideoCapture(input_path)\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out_full = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "    out_high = cv2.VideoWriter(highlight_path, fourcc, fps, (width, height))\n",
        "\n",
        "    tracker = RobustTracker(max_age=int(fps*3), reid_threshold=0.4, color_threshold=0.6)\n",
        "    orb = cv2.ORB_create()\n",
        "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
        "    prev_gray = None\n",
        "    prev_kps = None\n",
        "    prev_des = None\n",
        "\n",
        "    for frame_idx in tqdm(range(total_frames), desc='Processing Video'):\n",
        "        ret, frame = cap.read()\n",
        "        if not ret: break\n",
        "\n",
        "        # --- Camera Motion Compensation ---\n",
        "        motion_matrix = None\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "        kps, des = orb.detectAndCompute(gray, None)\n",
        "        if prev_gray is not None and des is not None and prev_des is not None:\n",
        "            matches = bf.match(prev_des, des)\n",
        "            if len(matches) > 50:\n",
        "                src_pts = np.float32([prev_kps[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
        "                dst_pts = np.float32([kps[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
        "                motion_matrix, _ = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
        "        prev_gray, prev_kps, prev_des = gray, kps, des\n",
        "\n",
        "        # --- Detection and Feature Extraction ---\n",
        "        results = detector(frame, classes=[0], verbose=False)\n",
        "        detections = results[0].boxes.xyxy.cpu().numpy()\n",
        "        \n",
        "        valid_detections, embeddings, color_hists = [], [], []\n",
        "        for i, (x1, y1, x2, y2) in enumerate(detections):\n",
        "            crop = frame[int(y1):int(y2), int(x1):int(x2)]\n",
        "            emb = get_embedding(crop)\n",
        "            hist = get_color_hist(crop)\n",
        "            if emb is not None and hist is not None:\n",
        "                embeddings.append(emb)\n",
        "                color_hists.append(hist)\n",
        "                valid_detections.append(detections[i])\n",
        "        \n",
        "        detections = np.array(valid_detections)\n",
        "        embeddings = np.array(embeddings)\n",
        "        color_hists = np.array(color_hists)\n",
        "\n",
        "        # --- Tracker Update ---\n",
        "        tracked_bboxes = tracker.update(detections, embeddings, color_hists, motion_matrix)\n",
        "\n",
        "        # --- Drawing ---\n",
        "        if tracked_bboxes:\n",
        "            for bbox in tracked_bboxes:\n",
        "                x1, y1, x2, y2 = map(int, bbox)\n",
        "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 3)\n",
        "                cv2.putText(frame, \"TARGET PLAYER\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
        "                out_high.write(frame)\n",
        "        \n",
        "        out_full.write(frame)\n",
        "\n",
        "    cap.release()\n",
        "    out_full.release()\n",
        "    out_high.release()\n",
        "    print(\"\\n--- Processing Complete ---\")\n",
        "    print(f\"‚úÖ Full video saved to: {output_path}\")\n",
        "    print(f\"‚úÖ Highlights video saved to: {highlight_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 5: Upload Video\n",
        "from google.colab import files\n",
        "\n",
        "print('üìπ Upload your video file (.mp4, .mov, etc.)...')\n",
        "uploaded_video = files.upload()\n",
        "\n",
        "input_video_path = None\n",
        "if uploaded_video:\n",
        "    input_video_path = f'/content/{next(iter(uploaded_video))}'\n",
        "    print(f\"\\n‚úÖ Video ready: {input_video_path}\")\n",
        "else:\n",
        "    print(\"\\n‚ùå No video file was uploaded. Please run this cell again.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 6: Run Processing and Download Results\n",
        "\n",
        "if input_video_path:\n",
        "    output_video_path = '/content/tracked_output.mp4'\n",
        "    highlight_video_path = '/content/highlights.mp4'\n",
        "    \n",
        "    process_video_robust(\n",
        "        input_path=input_video_path,\n",
        "        output_path=output_video_path,\n",
        "        highlight_path=highlight_video_path\n",
        "    )\n",
        "\n",
        "    print(\"\\n--- Download Your Files ---\")\n",
        "    try:\n",
        "        files.download(highlight_video_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Could not trigger automatic download. Please use the file browser on the left. Error: {e}\")\n",
        "else:\n",
        "    print(\"‚ùå Cannot run processing because no input video was found. Please upload a video in the cell above.\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
