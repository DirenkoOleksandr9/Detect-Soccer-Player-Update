{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elite Single-Player Re-Identification (v9.0 - Cerebrus Tracker with Continuity-Guided Re-ID)\n",
    "\n",
    "This version addresses the critical failure of the Re-ID system under motion. It introduces a new core logic: **Continuity-Guided Re-Identification**. This system now heavily prioritizes physical plausibility, ensuring the tracker can maintain a lock on a target even when their appearance changes due to motion blur or different angles.\n",
    "\n",
    "### Core Architecture Enhancements:\n",
    "- **Detector**: YOLOv8-Large\n",
    "- **Re-ID Model**: OpenCLIP ViT-H-14\n",
    "- **Cerebrus Tracking Algorithm v9**:\n",
    "  - **Continuity-Guided Re-ID (NEW)**: The tracker now calculates a 'continuity score' based on a candidate's proximity to the target's last known position. This score is fused with the visual similarity scores.\n",
    "  - **Intelligent Fused Score (NEW)**: The final matching decision is now based on a combination of visual similarity AND physical plausibility. The tracker will strongly prefer a candidate in a logical position, making it highly resilient to motion blur and pose changes.\n",
    "  - **Stable Visual Lock**: The system remains free of unstable predictive models, relying on direct visual evidence guided by the new continuity logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Install All Dependencies\n",
    "print(\"Installing system and Python dependencies...\")\n",
    "!sudo apt-get update -qq\n",
    "!sudo apt-get install -y tesseract-ocr -qq\n",
    "\n",
    "# Force-reinstall key libraries with compatible versions to avoid conflicts.\n",
    "!pip -q install \"numpy==1.26.4\" \"numba==0.58.1\" --force-reinstall\n",
    "!pip -q install \"torch>=2.3.0\" \"torchvision\" --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip -q install \"ultralytics>=8.0.0\" \"scipy>=1.10.0\" \"pytesseract>=0.3.10\" \"scikit-image>=0.21.0\"\n",
    "!pip -q install \"open-clip-torch>=2.24.0\"\n",
    "!pip -q install \"pillow>=10.0.0\" \"tqdm>=4.66.0\" \"opencv-python-headless>=4.9.0\" \"matplotlib\"\n",
    "\n",
    "print(\"\\n--- Verifying Installations ---\")\n",
    "try:\n",
    "    import numpy as np, cv2, torch, open_clip, ultralytics, scipy, pytesseract, skimage, matplotlib\n",
    "    from ultralytics import YOLO\n",
    "    print(f\"NumPy version: {np.__version__}\")\n",
    "    print(f\"OpenCV version: {cv2.__version__}\")\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"Tesseract OCR version: {pytesseract.get_tesseract_version()}\")\n",
    "    print(f\"Ultralytics (YOLOv8) version: {ultralytics.__version__}\")\n",
    "    print(\"All libraries loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during verification: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Initialize All Models & Advanced Utility Functions\n",
    "import torch, open_clip, cv2, pytesseract\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from typing import Optional, List, Dict, Any\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "print(\"Loading YOLOv8-Large model for player detection...\")\n",
    "player_detector = YOLO('yolov8l.pt')\n",
    "player_detector.to(device)\n",
    "\n",
    "print(\"Loading YOLOv8n model for ball detection...\")\n",
    "ball_detector = YOLO('yolov8n.pt')\n",
    "ball_detector.to(device)\n",
    "\n",
    "print(\"Loading OpenCLIP ViT-H/14 model for Re-ID...\")\n",
    "reid_model, _, reid_preprocess = open_clip.create_model_and_transforms('ViT-H-14', pretrained='laion2b_s32b_b79k', device=device)\n",
    "reid_model.eval()\n",
    "\n",
    "# --- Advanced Feature Extraction Functions (no changes) ---\n",
    "def get_embedding(crop_bgr: np.ndarray) -> Optional[np.ndarray]:\n",
    "    if crop_bgr.size == 0: return None\n",
    "    try:\n",
    "        rgb = cv2.cvtColor(crop_bgr, cv2.COLOR_BGR2RGB)\n",
    "        pil_img = Image.fromarray(rgb)\n",
    "        img_tensor = reid_preprocess(pil_img).unsqueeze(0).to(device)\n",
    "        with torch.no_grad(), torch.amp.autocast(device_type=device, dtype=torch.float16):\n",
    "            features = reid_model.encode_image(img_tensor)\n",
    "            features /= features.norm(dim=-1, keepdim=True)\n",
    "        return features.cpu().numpy().squeeze()\n",
    "    except Exception: return None\n",
    "\n",
    "def get_color_hist(crop_bgr: np.ndarray) -> Optional[np.ndarray]:\n",
    "    if crop_bgr.size == 0: return None\n",
    "    try:\n",
    "        lab_crop = cv2.cvtColor(crop_bgr, cv2.COLOR_BGR2LAB)\n",
    "        hist = cv2.calcHist([lab_crop], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])\n",
    "        cv2.normalize(hist, hist)\n",
    "        return hist.flatten()\n",
    "    except Exception: return None\n",
    "\n",
    "def get_ssim(crop1_bgr: np.ndarray, crop2_bgr: np.ndarray) -> float:\n",
    "    if crop1_bgr.size == 0 or crop2_bgr.size == 0: return 0.0\n",
    "    try:\n",
    "        h, w, _ = crop1_bgr.shape\n",
    "        crop2_resized = cv2.resize(crop2_bgr, (w, h))\n",
    "        gray1 = cv2.cvtColor(crop1_bgr, cv2.COLOR_BGR2GRAY)\n",
    "        gray2 = cv2.cvtColor(crop2_resized, cv2.COLOR_BGR2GRAY)\n",
    "        return ssim(gray1, gray2)\n",
    "    except Exception: return 0.0\n",
    "\n",
    "def ocr_jersey_number(crop_bgr: np.ndarray) -> Optional[str]:\n",
    "    if crop_bgr.size == 0 or crop_bgr.shape[0] < 30 or crop_bgr.shape[1] < 30: return None\n",
    "    try:\n",
    "        h, w, _ = crop_bgr.shape\n",
    "        torso = crop_bgr[int(h*0.2):int(h*0.8), int(w*0.2):int(w*0.8)]\n",
    "        gray = cv2.cvtColor(torso, cv2.COLOR_BGR2GRAY)\n",
    "        gray_resized = cv2.resize(gray, (150, 75), interpolation=cv2.INTER_CUBIC)\n",
    "        blurred = cv2.GaussianBlur(gray_resized, (3,3), 0)\n",
    "        sharpened = cv2.addWeighted(gray_resized, 1.5, blurred, -0.5, 0)\n",
    "        _, thresh = cv2.threshold(sharpened, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "        config = \"--psm 8 -c tessedit_char_whitelist=0123456789\"\n",
    "        text = pytesseract.image_to_string(thresh, config=config, timeout=1).strip()\n",
    "        return text if text.isdigit() else None\n",
    "    except Exception: return None\n",
    "\n",
    "print(\"All models and utility functions initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Define the Cerebrus Tracking System v9 with Continuity-Guided Re-ID\n",
    "from scipy.spatial.distance import cdist\n",
    "import collections\n",
    "\n",
    "def iou(boxA, boxB):\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
    "    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
    "    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
    "    iou_val = interArea / float(boxAArea + boxBArea - interArea)\n",
    "    return iou_val if not np.isnan(iou_val) else 0.0\n",
    "\n",
    "class CerebrusTrack:\n",
    "    def __init__(self, bbox, track_id, data, is_manual_init=False):\n",
    "        self.id = track_id\n",
    "        self.bbox = np.array(bbox)\n",
    "        self.time_since_update = 0\n",
    "        self.hits = 1\n",
    "        self.state = 'confirmed' if is_manual_init else 'tentative'\n",
    "        self.feature_ema = data['emb']\n",
    "        self.color_hist_ema = data['hist']\n",
    "        self.last_known_crop = data['crop']\n",
    "        self.jersey_number = data.get('jersey')\n",
    "        self.confirmed_number = self.jersey_number if is_manual_init else None\n",
    "        self.number_hits = collections.defaultdict(int)\n",
    "        if self.jersey_number: self.number_hits[self.jersey_number] += 5 if is_manual_init else 1\n",
    "\n",
    "    def update(self, bbox, data):\n",
    "        self.time_since_update = 0\n",
    "        self.hits += 1\n",
    "        self.bbox = np.array(bbox)\n",
    "        if self.state == 'lost': self.state = 'confirmed'\n",
    "        if self.hits > 5 and self.state == 'tentative': self.state = 'confirmed'\n",
    "        self.feature_ema = 0.9 * self.feature_ema + 0.1 * data['emb']\n",
    "        self.color_hist_ema = 0.9 * self.color_hist_ema + 0.1 * data['hist']\n",
    "        self.last_known_crop = data['crop']\n",
    "        if data.get('jersey'):\n",
    "            self.number_hits[data['jersey']] += 1\n",
    "            if self.number_hits[data['jersey']] > 5: self.confirmed_number = data['jersey']\n",
    "\n",
    "class CerebrusTracker:\n",
    "    def __init__(self, max_age=90):\n",
    "        self.max_age = max_age\n",
    "        # The base thresholds can be slightly more forgiving now\n",
    "        self.reid_thresh = 0.7\n",
    "        self.color_thresh = 0.6\n",
    "        self.ssim_thresh = 0.2\n",
    "        self.track_id_counter = 1\n",
    "        self.target_track = None\n",
    "        self.ball_pos = None\n",
    "\n",
    "    def initialize_target(self, initial_data):\n",
    "        self.target_track = CerebrusTrack(initial_data['bbox'], self.track_id_counter, initial_data, is_manual_init=True)\n",
    "        self.track_id_counter += 1\n",
    "\n",
    "    def update(self, frame, p_data, b_detections, target_jersey_number=None):\n",
    "        if b_detections.any():\n",
    "            best_ball = b_detections[np.argmax([(b[2]-b[0])*(b[3]-b[1]) for b in b_detections])]\n",
    "            self.ball_pos = (int((best_ball[0]+best_ball[2])/2), int((best_ball[1]+best_ball[3])/2))\n",
    "\n",
    "        if not self.target_track and target_jersey_number is not None:\n",
    "            if p_data:\n",
    "                found_player = None\n",
    "                if target_jersey_number:\n",
    "                    for d in p_data:\n",
    "                        if d.get('jersey') == target_jersey_number: found_player = d; break\n",
    "                else:\n",
    "                    pitch_center_x = frame.shape[1] / 2\n",
    "                    centrality = [abs((d['bbox'][0]+d['bbox'][2])/2 - pitch_center_x) for d in p_data]\n",
    "                    found_player = p_data[np.argmin(centrality)]\n",
    "                if found_player: self.target_track = CerebrusTrack(found_player['bbox'], self.track_id_counter, found_player)\n",
    "\n",
    "        if not self.target_track: return [], self.ball_pos\n",
    "\n",
    "        self.target_track.time_since_update += 1\n",
    "        if self.target_track.time_since_update > self.max_age:\n",
    "            self.target_track = None\n",
    "            return [], self.ball_pos\n",
    "        \n",
    "        match_found = False\n",
    "        if p_data:\n",
    "            last_box = self.target_track.bbox\n",
    "            w, h = last_box[2] - last_box[0], last_box[3] - last_box[1]\n",
    "            # Define a generous search area based on player size\n",
    "            search_area = [last_box[0] - w*1.5, last_box[1] - h*1.5, last_box[2] + w*1.5, last_box[3] + h*1.5]\n",
    "            \n",
    "            candidate_indices = [i for i, d in enumerate(p_data) if iou(d['bbox'], search_area) > 0]\n",
    "            \n",
    "            if candidate_indices:\n",
    "                candidate_detections = [p_data[i] for i in candidate_indices]\n",
    "                \n",
    "                reid_dists = cdist(np.array([self.target_track.feature_ema]), np.array([d['emb'] for d in candidate_detections]), 'cosine').flatten()\n",
    "                color_corrs = np.array([cv2.compareHist(self.target_track.color_hist_ema, d['hist'], cv2.HISTCMP_CORREL) for d in candidate_detections])\n",
    "                ssim_scores = np.array([get_ssim(self.target_track.last_known_crop, d['crop']) for d in candidate_detections])\n",
    "                \n",
    "                # --- Continuity Score Calculation ---\n",
    "                last_center = np.array([(last_box[0] + last_box[2]) / 2, (last_box[1] + last_box[3]) / 2])\n",
    "                candidate_centers = np.array([((d['bbox'][0] + d['bbox'][2]) / 2, (d['bbox'][1] + d['bbox'][3]) / 2) for d in candidate_detections])\n",
    "                # Normalize distance by player height to make it scale-invariant\n",
    "                distances = np.linalg.norm(candidate_centers - last_center, axis=1) / h\n",
    "                continuity_scores = np.exp(-distances) # Closer gets a score near 1.0, farther near 0.0\n",
    "\n",
    "                # --- Intelligent Fused Score ---\n",
    "                # Lower is better. We heavily weight the continuity score.\n",
    "                fused_scores = (reid_dists) + (1 - color_corrs) + (1 - ssim_scores) - (continuity_scores * 0.5)\n",
    "                best_candidate_idx = np.argmin(fused_scores)\n",
    "                \n",
    "                # Use a single, fused score threshold for a more holistic decision\n",
    "                if fused_scores[best_candidate_idx] < 1.5:\n",
    "                    original_idx = candidate_indices[best_candidate_idx]\n",
    "                    d = p_data[original_idx]\n",
    "                    self.target_track.update(d['bbox'], d)\n",
    "                    match_found = True\n",
    "\n",
    "        if not match_found:\n",
    "            self.target_track.state = 'lost'\n",
    "\n",
    "        output_tracks = [self.target_track] if self.target_track and self.target_track.state != 'tentative' else []\n",
    "        return output_tracks, self.ball_pos\n",
    "\n",
    "print(\"Cerebrus tracking system v9 defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4: Define the Main Video Processing Function\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def process_video_final(\n",
    "    input_path: str, output_path: str, highlight_path: str, mode: str,\n",
    "    initial_target_data: Optional[Dict[str, Any]] = None, target_jersey: Optional[str] = None\n",
    "):\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
    "    width, height = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    out_full = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "    out_high = cv2.VideoWriter(highlight_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "\n",
    "    tracker = CerebrusTracker(max_age=int(fps*3))\n",
    "    start_frame = 0\n",
    "    if mode == 'Manual' and initial_target_data:\n",
    "        tracker.initialize_target(initial_target_data)\n",
    "        cap.read()\n",
    "        start_frame = 1\n",
    "\n",
    "    for frame_idx in tqdm(range(start_frame, total_frames), desc='Processing Video'):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "\n",
    "        p_results = player_detector(frame, classes=[0], verbose=False, conf=0.4)\n",
    "        p_detections = p_results[0].boxes.xyxy.cpu().numpy()\n",
    "        b_results = ball_detector(frame, classes=[32], verbose=False, conf=0.5)\n",
    "        b_detections = b_results[0].boxes.xyxy.cpu().numpy()\n",
    "        \n",
    "        p_data = []\n",
    "        for i, (x1, y1, x2, y2) in enumerate(p_detections):\n",
    "            crop = frame[int(y1):int(y2), int(x1):int(x2)]\n",
    "            emb, hist = get_embedding(crop), get_color_hist(crop)\n",
    "            if emb is not None and hist is not None:\n",
    "                p_data.append({'bbox': p_detections[i], 'emb': emb, 'hist': hist, 'jersey': ocr_jersey_number(crop), 'crop': crop})\n",
    "        \n",
    "        auto_target = target_jersey if mode == 'Automatic' else None\n",
    "        if mode == 'Automatic' and target_jersey == '': auto_target = ''\n",
    "        \n",
    "        tracked_players, ball_pos = tracker.update(frame, p_data, b_detections, target_jersey_number=auto_target)\n",
    "\n",
    "        is_highlight_frame = False\n",
    "        if tracked_players:\n",
    "            track = tracked_players[0]\n",
    "            x1, y1, x2, y2 = map(int, track.bbox)\n",
    "            label = f\"PLAYER {track.confirmed_number}\" if track.confirmed_number else \"TARGET PLAYER\"\n",
    "            state_label = f\"STATE: {track.state.upper()}\"\n",
    "            \n",
    "            color = (0, 255, 0)\n",
    "            if track.state == 'lost': \n",
    "                color = (0, 165, 255)\n",
    "            elif track.state == 'tentative': \n",
    "                color = (255, 255, 0)\n",
    "            \n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 3)\n",
    "            cv2.putText(frame, label, (x1, y1 - 35), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "            cv2.putText(frame, state_label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "            is_highlight_frame = True\n",
    "\n",
    "        if ball_pos:\n",
    "             cv2.circle(frame, ball_pos, 15, (0, 255, 255), -1)\n",
    "             cv2.circle(frame, ball_pos, 15, (0,0,0), 2)\n",
    "             if tracked_players:\n",
    "                 player_center = ((x1+x2)//2, (y1+y2)//2)\n",
    "                 if np.linalg.norm(np.array(player_center) - np.array(ball_pos)) < 250: is_highlight_frame = True\n",
    "\n",
    "        if is_highlight_frame:\n",
    "            out_high.write(frame)\n",
    "        \n",
    "        out_full.write(frame)\n",
    "\n",
    "    cap.release(); out_full.release(); out_high.release()\n",
    "    print(\"\\n--- Processing Complete ---\")\n",
    "    print(f\"Full video saved to: {output_path}\")\n",
    "    print(f\"Highlights video saved to: {highlight_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5: Upload Video and Select Tracking Mode & Target\n",
    "from google.colab import files\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "\n",
    "print('Upload your video file (.mp4, .mov, etc.)...')\n",
    "uploaded_video = files.upload()\n",
    "input_video_path = None\n",
    "first_frame_data = []\n",
    "selected_player_data = None\n",
    "TARGET_JERSEY_NUMBER = ''\n",
    "\n",
    "#@markdown ### 1. Choose Tracking Mode\n",
    "TRACKING_MODE = 'Manual' #@param [\"Manual\", \"Automatic\"]\n",
    "\n",
    "if uploaded_video:\n",
    "    input_video_path = f'/content/{next(iter(uploaded_video))}'\n",
    "    print(f\"\\nVideo ready: {input_video_path}\")\n",
    "    \n",
    "    if TRACKING_MODE == 'Manual':\n",
    "        cap = cv2.VideoCapture(input_video_path)\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            p_results = player_detector(frame, classes=[0], verbose=False, conf=0.3)\n",
    "            p_detections = p_results[0].boxes.xyxy.cpu().numpy()\n",
    "            for i, (x1, y1, x2, y2) in enumerate(p_detections):\n",
    "                crop = frame[int(y1):int(y2), int(x1):int(x2)]\n",
    "                emb, hist = get_embedding(crop), get_color_hist(crop)\n",
    "                if emb is not None and hist is not None:\n",
    "                    first_frame_data.append({'id': i + 1, 'bbox': p_detections[i], 'emb': emb, 'hist': hist, 'jersey': ocr_jersey_number(crop), 'crop': crop})\n",
    "            \n",
    "            annotated_frame = frame.copy()\n",
    "            for player in first_frame_data:\n",
    "                x1, y1, x2, y2 = map(int, player['bbox'])\n",
    "                cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                cv2.putText(annotated_frame, f\"ID: {player['id']}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0,0,255), 3)\n",
    "            \n",
    "            print(\"\\n--- SELECT YOUR TARGET PLAYER ---\")\n",
    "            annotated_frame_rgb = cv2.cvtColor(annotated_frame, cv2.COLOR_BGR2RGB)\n",
    "            plt.figure(figsize=(16, 9)); plt.imshow(annotated_frame_rgb); plt.axis('off'); plt.title('Enter the ID of the player you want to track below'); plt.show()\n",
    "        cap.release()\n",
    "    else:\n",
    "        print(\"\\n--- AUTOMATIC MODE SELECTED ---\")\n",
    "        print(\"Configure jersey number below. Leave blank to track the most central player.\")\n",
    "else:\n",
    "    print(\"\\nNo video file was uploaded. Please run this cell again.\")\n",
    "\n",
    "#@markdown --- \n",
    "#@markdown ### 2. Configure Target (based on mode)\n",
    "#@markdown - If **Manual**, enter the ID of the Target Player from the image above.\n",
    "#@markdown - If **Automatic**, (optional) enter the Jersey Number to track.\n",
    "TARGET_PLAYER_ID = 1 #@param {type:\"integer\"}\n",
    "TARGET_JERSEY_NUMBER = \"\" #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 6: Run Processing and Download Results\n",
    "\n",
    "if input_video_path:\n",
    "    output_video_path = '/content/tracked_output_cerebrus_v9.mp4'\n",
    "    highlight_video_path = '/content/highlights_cerebrus_v9.mp4'\n",
    "    \n",
    "    if TRACKING_MODE == 'Manual':\n",
    "        if not first_frame_data:\n",
    "            print(\"Error: Manual mode selected but no players were detected in the first frame. Try Automatic mode.\")\n",
    "        else:\n",
    "            selected_player_data = next((p for p in first_frame_data if p['id'] == TARGET_PLAYER_ID), None)\n",
    "            if selected_player_data:\n",
    "                print(f\"Target locked: Player ID {TARGET_PLAYER_ID}. Starting tracking...\")\n",
    "                process_video_final(\n",
    "                    input_path=input_video_path, output_path=output_video_path, highlight_path=highlight_video_path,\n",
    "                    mode='Manual', initial_target_data=selected_player_data\n",
    "                )\n",
    "            else:\n",
    "                print(f\"Error: Player with ID {TARGET_PLAYER_ID} not found. Please check the ID and run the cell again.\")\n",
    "    \n",
    "    elif TRACKING_MODE == 'Automatic':\n",
    "        if TARGET_JERSEY_NUMBER:\n",
    "            print(f\"Attempting to automatically track player with jersey number: {TARGET_JERSEY_NUMBER}\")\n",
    "        else:\n",
    "            print(\"No jersey number specified. Will automatically track the most central player.\")\n",
    "        \n",
    "        process_video_final(\n",
    "            input_path=input_video_path, output_path=output_video_path, highlight_path=highlight_video_path,\n",
    "            mode='Automatic', target_jersey=TARGET_JERSEY_NUMBER\n",
    "        )\n",
    "\n",
    "    import os\n",
    "    if os.path.exists(output_video_path):\n",
    "        print(\"\\n--- Download Your Files ---\")\n",
    "        try:\n",
    "            print(\"Downloading highlights video...\")\n",
    "            files.download(highlight_video_path)\n",
    "            print(\"Downloading full tracked video...\")\n",
    "            files.download(output_video_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not trigger automatic download. Error: {e}\")\n",
    "else:\n",
    "    print(\"Cannot run processing. Please upload a video and select a mode in the cell above.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
