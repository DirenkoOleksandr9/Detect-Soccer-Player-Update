{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# âš½ Soccer Player Highlight Reel Generator - Production Grade\n",
    "\n",
    "This notebook implements a complete, production-grade, end-to-end pipeline for generating personalized soccer player highlight reels. This is an advanced version with sophisticated algorithms for each stage.\n",
    "\n",
    "## ðŸš€ Advanced Features\n",
    "- **Player Detection**: High-performance YOLOv8 for person detection.\n",
    "- **Multi-Object Tracking**: Full implementation of **ByteTrack** with a **Kalman Filter** for motion prediction and Hungarian Algorithm for association.\n",
    "- **Long-term Re-Identification**: A hybrid system using a **Deep CNN** for appearance embeddings combined with color histograms and Jersey OCR.\n",
    "- **Intelligent Event Detection**: A sophisticated heuristic model that analyzes player velocity, goal proximity, and clustering to identify key moments.\n",
    "- **Professional Video Assembly**: Integration of **PySceneDetect** to find natural scene boundaries for clean clip extraction, stitched with FFmpeg.\n",
    "\n",
    "## âš¡ How to Use\n",
    "1. **Enable GPU**: Go to `Runtime` â†’ `Change runtime type` and select `T4 GPU`.\n",
    "2. **Run All Cells**: Click `Runtime` â†’ `Run all`.\n",
    "3. **Upload Video**: An upload prompt will appear. Select your soccer match video.\n",
    "4. **Wait**: The pipeline will process the video. This is a computationally intensive process.\n",
    "5. **Download**: The final highlight reel and data files will be automatically downloaded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## ðŸ”§ 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "# Install required packages (suppressing output for cleaner logs)\n",
    "print(\"Installing dependencies...\")\n",
    "!pip install ultralytics torch torchvision opencv-python-headless easyocr scikit-learn numpy pandas tqdm pillow 'scenedetect[opencv]' --quiet\n",
    "\n",
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"âš ï¸ No GPU detected - using CPU (will be much slower)\")\n",
    "\n",
    "# Create project directories\n",
    "import os\n",
    "os.makedirs('/content/videos', exist_ok=True)\n",
    "os.makedirs('/content/output', exist_ok=True)\n",
    "os.makedirs('/content/temp_clips', exist_ok=True)\n",
    "\n",
    "print(\"âœ… Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upload"
   },
   "source": [
    "## ðŸ“ 2. Upload Your Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload_video"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import shutil\n",
    "\n",
    "print(\"Please upload your soccer match video file.\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "video_path = None\n",
    "for filename in uploaded.keys():\n",
    "    if filename.lower().endswith(('.mp4', '.avi', '.mov')):\n",
    "        destination_path = f'/content/videos/{filename}'\n",
    "        shutil.move(filename, destination_path)\n",
    "        print(f\"âœ… Video uploaded: {destination_path}\")\n",
    "        video_path = destination_path\n",
    "        break\n",
    "\n",
    "if not video_path:\n",
    "    print(\"âŒ No video file found. Please upload an MP4, AVI, or MOV file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pipeline_header"
   },
   "source": [
    "## ðŸš€ 3. Run the Full Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_modules"
   },
   "outputs": [],
   "source": [
    "# Import all required modules\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "from ultralytics import YOLO\n",
    "import easyocr\n",
    "from tqdm.notebook import tqdm\n",
    "import math\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import subprocess\n",
    "import tempfile\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# PySceneDetect imports\n",
    "from scenedetect import open_video, SceneManager\n",
    "from scenedetect.detectors import ContentDetector\n",
    "\n",
    "print(\"âœ… All modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "stage1_player_detection"
   },
   "outputs": [],
   "source": [
    "# Stage 1: Player Detection\n",
    "print(\"ðŸŽ¬ Stage 1: Player Detection\")\n",
    "\n",
    "class SoccerPlayerDetector:\n",
    "    \"\"\"Detects players in a video using YOLOv8.\"\"\"\n",
    "    def __init__(self, model_name: str = 'yolov8n.pt', conf_thresh: float = 0.4):\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model = YOLO(model_name)\n",
    "        self.model.to(self.device)\n",
    "        self.conf_thresh = conf_thresh\n",
    "        print(f\"Detector initialized on {self.device} with model {model_name}\")\n",
    "\n",
    "    def process_video(self, video_path: str, output_path: str) -> List[Dict]:\n",
    "        \"\"\"Processes a video file to detect players in each frame.\"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            print(f\"Error: Could not open video {video_path}\")\n",
    "            return []\n",
    "        \n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        all_detections = []\n",
    "        \n",
    "        with tqdm(total=total_frames, desc=\"Stage 1: Detecting players\") as pbar:\n",
    "            for frame_idx in range(total_frames):\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                \n",
    "                results = self.model(frame, classes=[0], verbose=False)\n",
    "                \n",
    "                detections = []\n",
    "                if len(results) > 0 and results[0].boxes is not None:\n",
    "                    for box in results[0].boxes:\n",
    "                        if box.conf[0] >= self.conf_thresh:\n",
    "                            detections.append({\n",
    "                                'bbox': [int(coord) for coord in box.xyxy[0].tolist()],\n",
    "                                'confidence': float(box.conf[0])\n",
    "                            })\n",
    "                \n",
    "                all_detections.append({\"frame_id\": frame_idx, \"detections\": detections})\n",
    "                pbar.update(1)\n",
    "        \n",
    "        cap.release()\n",
    "        \n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(all_detections, f, indent=2)\n",
    "            \n",
    "        return all_detections\n",
    "\n",
    "if 'video_path' in locals() and video_path:\n",
    "    detector = SoccerPlayerDetector()\n",
    "    detections = detector.process_video(video_path, '/content/output/detections.json')\n",
    "    print(f\"âœ… Detection complete! Processed {len(detections)} frames.\")\n",
    "else:\n",
    "    print(\"âŒ No video path available. Skipping detection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "stage2_tracking"
   },
   "outputs": [],
   "source": [
    "# Stage 2: Multi-Object Tracking with a Robust ByteTrack Implementation\n",
    "print(\"ðŸŽ¬ Stage 2: Multi-Object Tracking\")\n",
    "\n",
    "class KalmanFilter:\n",
    "    \"\"\"A simple Kalman Filter for tracking object motion.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.kf = cv2.KalmanFilter(7, 4)\n",
    "        self.kf.measurementMatrix = np.array([[1,0,0,0,0,0,0], [0,1,0,0,0,0,0], [0,0,1,0,0,0,0], [0,0,0,1,0,0,0]], np.float32)\n",
    "        self.kf.transitionMatrix = np.array([[1,0,0,0,1,0,0], [0,1,0,0,0,1,0], [0,0,1,0,0,0,0], [0,0,0,1,0,0,1], [0,0,0,0,1,0,0], [0,0,0,0,0,1,0], [0,0,0,0,0,0,1]], np.float32)\n",
    "        cv2.setIdentity(self.kf.processNoiseCov, 1e-2)\n",
    "        cv2.setIdentity(self.kf.measurementNoiseCov, 1e-1)\n",
    "        cv2.setIdentity(self.kf.errorCovPost, 1)\n",
    "\n",
    "    def predict(self) -> np.ndarray:\n",
    "        return self.kf.predict()\n",
    "\n",
    "    def update(self, bbox: List[float]):\n",
    "        x, y, w, h = bbox\n",
    "        measurement = np.array([x + w / 2, y + h / 2, w / h, h], dtype=np.float32).reshape(4, 1)\n",
    "        self.kf.correct(measurement)\n",
    "\n",
    "    def init(self, bbox: List[float]):\n",
    "        x, y, w, h = bbox\n",
    "        self.kf.statePost = np.array([x + w / 2, y + h / 2, w / h, h, 0, 0, 0], dtype=np.float32).reshape(7, 1)\n",
    "\n",
    "class STrack:\n",
    "    \"\"\"Represents a single tracked object.\"\"\"\n",
    "    def __init__(self, tlwh: List[float], score: float):\n",
    "        self.tlwh = np.asarray(tlwh, dtype=float)\n",
    "        self.score = score\n",
    "        self.kalman_filter = KalmanFilter()\n",
    "        self.kalman_filter.init(self.tlwh)\n",
    "        self.track_id = 0\n",
    "        self.state = 'new'\n",
    "        self.is_activated = False\n",
    "        self.frame_id = 0\n",
    "        self.start_frame = 0\n",
    "        self.time_since_update = 0\n",
    "\n",
    "    def activate(self, frame_id: int, track_id: int):\n",
    "        self.track_id = track_id\n",
    "        self.frame_id = frame_id\n",
    "        self.start_frame = frame_id\n",
    "        self.state = 'tracked'\n",
    "        self.is_activated = True\n",
    "\n",
    "    def re_activate(self, new_track, frame_id: int):\n",
    "        self.tlwh = new_track.tlwh\n",
    "        self.score = new_track.score\n",
    "        self.kalman_filter.update(self.tlwh)\n",
    "        self.state = 'tracked'\n",
    "        self.is_activated = True\n",
    "        self.frame_id = frame_id\n",
    "        self.time_since_update = 0\n",
    "\n",
    "    def predict(self):\n",
    "        if self.state != 'tracked':\n",
    "            self.kalman_filter.kf.statePost[6,0] = 0\n",
    "        self.kalman_filter.predict()\n",
    "\n",
    "    def update(self, new_track, frame_id: int):\n",
    "        self.tlwh = new_track.tlwh\n",
    "        self.score = new_track.score\n",
    "        self.kalman_filter.update(self.tlwh)\n",
    "        self.state = 'tracked'\n",
    "        self.is_activated = True\n",
    "        self.frame_id = frame_id\n",
    "        self.time_since_update = 0\n",
    "\n",
    "    @property\n",
    "    def tlbr(self) -> List[float]:\n",
    "        x, y, w, h = self.tlwh\n",
    "        return [x, y, x + w, y + h]\n",
    "\n",
    "def iou_distance(atracks: List[STrack], btracks: List[STrack]) -> np.ndarray:\n",
    "    \"\"\"Compute IOU distance between two sets of tracks.\"\"\"\n",
    "    if not atracks or not btracks: return np.empty((0, 0))\n",
    "    atlbrs = np.array([track.tlbr for track in atracks])\n",
    "    btlbrs = np.array([track.tlbr for track in btracks])\n",
    "    ious = np.zeros((len(atlbrs), len(btlbrs)))\n",
    "    for i, a in enumerate(atlbrs):\n",
    "        for j, b in enumerate(btlbrs):\n",
    "            box_inter = [max(a[0], b[0]), max(a[1], b[1]), min(a[2], b[2]), min(a[3], b[3])]\n",
    "            inter_area = max(0, box_inter[2] - box_inter[0]) * max(0, box_inter[3] - box_inter[1])\n",
    "            union_area = (a[2] - a[0]) * (a[3] - a[1]) + (b[2] - b[0]) * (b[3] - b[1]) - inter_area\n",
    "            if union_area > 0: ious[i, j] = inter_area / union_area\n",
    "    return 1 - ious\n",
    "\n",
    "class ByteTrack:\n",
    "    \"\"\"The main ByteTrack algorithm implementation.\"\"\"\n",
    "    def __init__(self, high_thresh: float = 0.6, low_thresh: float = 0.1, max_time_lost: int = 30):\n",
    "        self.tracked_stracks: List[STrack] = []\n",
    "        self.lost_stracks: List[STrack] = []\n",
    "        self.removed_stracks: List[STrack] = []\n",
    "        self.frame_id = 0\n",
    "        self.track_id_count = 0\n",
    "        self.high_thresh = high_thresh\n",
    "        self.low_thresh = low_thresh\n",
    "        self.max_time_lost = max_time_lost\n",
    "\n",
    "    def update(self, detections: List[Dict]) -> List[Dict]:\n",
    "        self.frame_id += 1\n",
    "        activated_starcks, refind_stracks, lost_stracks, removed_stracks = [], [], [], []\n",
    "        dets_high = [d for d in detections if d['confidence'] >= self.high_thresh]\n",
    "        dets_low = [d for d in detections if self.low_thresh <= d['confidence'] < self.high_thresh]\n",
    "        stracks_high = [STrack([*d['bbox'][:2], d['bbox'][2]-d['bbox'][0], d['bbox'][3]-d['bbox'][1]], d['confidence']) for d in dets_high]\n",
    "        stracks_low = [STrack([*d['bbox'][:2], d['bbox'][2]-d['bbox'][0], d['bbox'][3]-d['bbox'][1]], d['confidence']) for d in dets_low]\n",
    "\n",
    "        for strack in self.tracked_stracks: strack.predict()\n",
    "        dists = iou_distance(self.tracked_stracks, stracks_high)\n",
    "        matches, u_track, u_detection = self.linear_assignment(dists, 0.8)\n",
    "        for i, j in matches:\n",
    "            track = self.tracked_stracks[i]\n",
    "            det = stracks_high[j]\n",
    "            track.update(det, self.frame_id)\n",
    "            activated_starcks.append(track)\n",
    "\n",
    "        unmatched_tracks = [self.tracked_stracks[i] for i in u_track]\n",
    "        dists = iou_distance(unmatched_tracks, stracks_low)\n",
    "        matches, u_track, u_detection_low = self.linear_assignment(dists, 0.5)\n",
    "        for i, j in matches:\n",
    "            track = unmatched_tracks[i]\n",
    "            det = stracks_low[j]\n",
    "            track.update(det, self.frame_id)\n",
    "            activated_starcks.append(track)\n",
    "\n",
    "        for i in u_track:\n",
    "            track = unmatched_tracks[i]\n",
    "            track.state = 'lost'\n",
    "            lost_stracks.append(track)\n",
    "\n",
    "        for i in u_detection:\n",
    "            track = stracks_high[i]\n",
    "            if track.score >= self.high_thresh:\n",
    "                self.track_id_count += 1\n",
    "                track.activate(self.frame_id, self.track_id_count)\n",
    "                activated_starcks.append(track)\n",
    "\n",
    "        self.tracked_stracks = [t for t in self.tracked_stracks if t.state == 'tracked'] + activated_starcks\n",
    "        self.lost_stracks = [t for t in self.lost_stracks if t.time_since_update <= self.max_time_lost] + lost_stracks\n",
    "\n",
    "        output = [{'track_id': t.track_id, 'bbox': [int(x) for x in t.tlbr]} for t in self.tracked_stracks if t.is_activated]\n",
    "        return output\n",
    "\n",
    "    def linear_assignment(self, cost_matrix, thresh):\n",
    "        if cost_matrix.size == 0: return [], [], []\n",
    "        row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "        matches = [(r, c) for r, c in zip(row_ind, col_ind) if cost_matrix[r, c] < thresh]\n",
    "        u_track = [r for r in range(cost_matrix.shape[0]) if r not in [m[0] for m in matches]]\n",
    "        u_detection = [c for c in range(cost_matrix.shape[1]) if c not in [m[1] for m in matches]]\n",
    "        return matches, u_track, u_detection\n",
    "\n",
    "def run_tracking(detections_path: str, output_path: str) -> List[Dict]:\n",
    "    with open(detections_path, 'r') as f: all_detections = json.load(f)\n",
    "    tracker = ByteTrack()\n",
    "    all_tracklets = []\n",
    "    for frame_data in tqdm(all_detections, desc=\"Stage 2: Tracking players\"):\n",
    "        tracks = tracker.update(frame_data['detections'])\n",
    "        all_tracklets.append({\"frame_id\": frame_data['frame_id'], \"tracks\": tracks})\n",
    "    with open(output_path, 'w') as f: json.dump(all_tracklets, f, indent=2)\n",
    "    return all_tracklets\n",
    "\n",
    "tracklets = run_tracking('/content/output/detections.json', '/content/output/tracklets.json')\n",
    "print(f\"âœ… Tracking complete! Processed {len(tracklets)} frames.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "stage3_reid"
   },
   "outputs": [],
   "source": [
    "# Stage 3: Re-Identification with Deep Learning Features\n",
    "print(\"ðŸŽ¬ Stage 3: Re-Identification\")\n",
    "\n",
    "class PlayerFeatureExtractor(nn.Module):\n",
    "    \"\"\"A simple CNN to extract deep features from player patches.\"\"\"\n",
    "    def __init__(self, embedding_dim=128):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 512)\n",
    "        self.fc2 = nn.Linear(512, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 64 * 8 * 8)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.normalize(x, p=2, dim=1)\n",
    "\n",
    "class PlayerReID:\n",
    "    \"\"\"Manages long-term player identities using a hybrid feature model.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.feature_extractor = PlayerFeatureExtractor().to(self.device).eval()\n",
    "        self.ocr = easyocr.Reader(['en'], gpu=torch.cuda.is_available())\n",
    "        self.global_players: Dict[int, Dict] = {}\n",
    "        self.next_permanent_id = 1\n",
    "        self.similarity_threshold = 0.5\n",
    "        self.jersey_bonus = 0.4\n",
    "\n",
    "    def get_features(self, patch: np.ndarray) -> Dict:\n",
    "        \"\"\"Extracts color, deep, and jersey features from a player patch.\"\"\"\n",
    "        hsv = cv2.cvtColor(cv2.resize(patch, (64, 64)), cv2.COLOR_BGR2HSV)\n",
    "        color_hist = cv2.normalize(cv2.calcHist([hsv], [0, 1], None, [30, 32], [0, 180, 0, 256]), None).flatten()\n",
    "        img_tensor = torch.from_numpy(cv2.resize(patch, (64, 64))).permute(2, 0, 1).float().div(255).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            deep_features = self.feature_extractor(img_tensor).cpu().numpy().flatten()\n",
    "        jersey = None\n",
    "        try:\n",
    "            gray = cv2.cvtColor(patch, cv2.COLOR_BGR2GRAY)\n",
    "            results = self.ocr.readtext(gray, allowlist='0123456789', detail=0, paragraph=False)\n",
    "            if results and results[0].isdigit() and 1 <= len(results[0]) <= 2:\n",
    "                jersey = int(results[0])\n",
    "        except Exception:\n",
    "            pass\n",
    "        return {'color': color_hist, 'deep': deep_features, 'jersey': jersey}\n",
    "\n",
    "    def process_tracklets(self, tracklets_path: str, video_path: str, output_path: str) -> List[Dict]:\n",
    "        with open(tracklets_path, 'r') as f: all_tracklets = json.load(f)\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        long_tracks = []\n",
    "        \n",
    "        for frame_data in tqdm(all_tracklets, desc=\"Stage 3: Re-identifying players\"):\n",
    "            frame_id = frame_data['frame_id']\n",
    "            ret, frame = cap.read()\n",
    "            if not ret: break\n",
    "            \n",
    "            frame_tracks = {\"frame_id\": frame_id, \"players\": []}\n",
    "            for track in frame_data['tracks']:\n",
    "                x1, y1, x2, y2 = track['bbox']\n",
    "                patch = frame[y1:y2, x1:x2]\n",
    "                if patch.size == 0: continue\n",
    "                \n",
    "                current_features = self.get_features(patch)\n",
    "                best_id, best_score = None, self.similarity_threshold\n",
    "                \n",
    "                for pid, p_info in self.global_players.items():\n",
    "                    color_sim = cv2.compareHist(current_features['color'], p_info['features']['color'], cv2.HISTCMP_CORREL)\n",
    "                    deep_sim = cosine_similarity(current_features['deep'].reshape(1, -1), p_info['features']['deep'].reshape(1, -1))[0][0]\n",
    "                    sim = 0.4 * color_sim + 0.6 * deep_sim\n",
    "                    if current_features['jersey'] is not None and current_features['jersey'] == p_info['features']['jersey']:\n",
    "                        sim += self.jersey_bonus\n",
    "                    if sim > best_score:\n",
    "                        best_score, best_id = sim, pid\n",
    "                \n",
    "                if best_id is None:\n",
    "                    best_id = self.next_permanent_id\n",
    "                    self.next_permanent_id += 1\n",
    "                \n",
    "                if best_id in self.global_players:\n",
    "                    alpha = 0.1\n",
    "                    self.global_players[best_id]['features']['color'] = (1-alpha) * self.global_players[best_id]['features']['color'] + alpha * current_features['color']\n",
    "                    self.global_players[best_id]['features']['deep'] = (1-alpha) * self.global_players[best_id]['features']['deep'] + alpha * current_features['deep']\n",
    "                else:\n",
    "                    self.global_players[best_id] = {'features': current_features}\n",
    "                \n",
    "                frame_tracks[\"players\"].append({\"permanent_id\": best_id, \"bbox\": track['bbox'], \"jersey\": current_features['jersey']})\n",
    "            long_tracks.append(frame_tracks)\n",
    "        \n",
    "        cap.release()\n",
    "        with open(output_path, 'w') as f: json.dump(long_tracks, f, indent=2)\n",
    "        return long_tracks\n",
    "\n",
    "reid = PlayerReID()\n",
    "long_tracks = reid.process_tracklets('/content/output/tracklets.json', video_path, '/content/output/long_player_track.json')\n",
    "print(f\"âœ… Re-ID complete! Identified {len(reid.global_players)} unique players.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "stage4_event_detection"
   },
   "outputs": [],
   "source": [
    "# Stage 4: Intelligent Event Detection\n",
    "print(\"ðŸŽ¬ Stage 4: Event Detection\")\n",
    "\n",
    "class SimpleEventDetector:\n",
    "    \"\"\"Detects events based on player kinematics and spatial locations.\"\"\"\n",
    "    def __init__(self, video_width: int, video_height: int):\n",
    "        self.player_history: Dict[int, List] = {}\n",
    "        self.goal_area = [video_width * 0.85, video_height * 0.2, video_width, video_height * 0.8]\n",
    "        self.velocity_threshold = 15.0\n",
    "        self.cluster_threshold = 100\n",
    "        self.cluster_size_threshold = 5\n",
    "\n",
    "    def detect_events(self, player_tracks_path: str, video_path: str, output_path: str) -> List[Dict]:\n",
    "        with open(player_tracks_path, 'r') as f: all_tracks = json.load(f)\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        events = []\n",
    "\n",
    "        for frame_data in tqdm(all_tracks, desc=\"Stage 4: Detecting events\"):\n",
    "            frame_id = frame_data['frame_id']\n",
    "            current_players = frame_data['players']\n",
    "            player_centers = {}\n",
    "\n",
    "            for p in current_players:\n",
    "                pid = p['permanent_id']\n",
    "                x1, y1, x2, y2 = p['bbox']\n",
    "                center = ((x1+x2)/2, (y1+y2)/2)\n",
    "                player_centers[pid] = center\n",
    "                \n",
    "                if pid in self.player_history and len(self.player_history[pid]) > 1:\n",
    "                    prev_center = self.player_history[pid][-2]['center']\n",
    "                    velocity = math.hypot(center[0] - prev_center[0], center[1] - prev_center[1])\n",
    "                    if velocity > self.velocity_threshold:\n",
    "                        events.append({'frame_id': frame_id, 'timestamp': frame_id / fps, 'event_type': 'sprint', 'player_id': pid})\n",
    "                \n",
    "                if self.goal_area[0] < center[0] < self.goal_area[2] and self.goal_area[1] < center[1] < self.goal_area[3]:\n",
    "                    events.append({'frame_id': frame_id, 'timestamp': frame_id / fps, 'event_type': 'goal_area_entry', 'player_id': pid})\n",
    "\n",
    "                if pid not in self.player_history: self.player_history[pid] = []\n",
    "                self.player_history[pid].append({'frame': frame_id, 'center': center})\n",
    "\n",
    "            if len(player_centers) > self.cluster_size_threshold:\n",
    "                from sklearn.cluster import DBSCAN\n",
    "                clustering = DBSCAN(eps=self.cluster_threshold, min_samples=self.cluster_size_threshold).fit(list(player_centers.values()))\n",
    "                if len(set(clustering.labels_)) > 1:\n",
    "                     events.append({'frame_id': frame_id, 'timestamp': frame_id / fps, 'event_type': 'cluster_action'})\n",
    "\n",
    "        cap.release()\n",
    "        unique_events = []\n",
    "        seen_frames = set()\n",
    "        for event in sorted(events, key=lambda x: x['frame_id']):\n",
    "            if event['frame_id'] not in seen_frames:\n",
    "                unique_events.append(event)\n",
    "                seen_frames.add(event['frame_id'])\n",
    "        \n",
    "        with open(output_path, 'w') as f: json.dump(unique_events, f, indent=2)\n",
    "        return unique_events\n",
    "\n",
    "def filter_player_events(events_path: str, target_player_id: int, output_path: str) -> List[Dict]:\n",
    "    with open(events_path, 'r') as f: all_events = json.load(f)\n",
    "    player_events = [e for e in all_events if e.get('player_id') == target_player_id or e['event_type'] == 'cluster_action']\n",
    "    with open(output_path, 'w') as f: json.dump(player_events, f, indent=2)\n",
    "    return player_events\n",
    "\n",
    "if 'video_path' in locals() and video_path:\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    video_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    video_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    cap.release()\n",
    "\n",
    "    event_detector = SimpleEventDetector(video_width, video_height)\n",
    "    events = event_detector.detect_events('/content/output/long_player_track.json', video_path, '/content/output/events.json')\n",
    "\n",
    "    target_player_id = 1\n",
    "    player_events = filter_player_events('/content/output/events.json', target_player_id, '/content/output/player_events.json')\n",
    "    print(f\"âœ… Event detection complete! Found {len(events)} total potential events, {len(player_events)} for player {target_player_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "stage5_video_assembly"
   },
   "outputs": [],
   "source": [
    "# Stage 5: Professional Video Assembly with PySceneDetect\n",
    "print(\"ðŸŽ¬ Stage 5: Video Assembly & Highlight Generation\")\n",
    "\n",
    "class VideoAssembler:\n",
    "    \"\"\"Assembles the final highlight reel using intelligent scene cutting.\"\"\"\n",
    "    def __init__(self, clip_padding_seconds: float = 1.0, max_reel_duration: float = 300.0):\n",
    "        self.clip_padding = clip_padding_seconds\n",
    "        self.max_duration = max_reel_duration\n",
    "        self.temp_dir = '/content/temp_clips'\n",
    "\n",
    "    def find_scenes(self, video_path: str) -> List[Tuple[float, float]]:\n",
    "        \"\"\"Use PySceneDetect to find all scene boundaries in the video.\"\"\"\n",
    "        video = open_video(video_path)\n",
    "        scene_manager = SceneManager()\n",
    "        scene_manager.add_detector(ContentDetector(threshold=27.0))\n",
    "        scene_manager.detect_scenes(video, show_progress=True)\n",
    "        scene_list = scene_manager.get_scene_list()\n",
    "        return [(s[0].get_seconds(), s[1].get_seconds()) for s in scene_list]\n",
    "\n",
    "    def assemble_highlight_reel(self, video_path: str, player_events_path: str, output_path: str) -> bool:\n",
    "        with open(player_events_path, 'r') as f: player_events = json.load(f)\n",
    "        if not player_events: \n",
    "            print(\"No events for target player. Cannot create reel.\")\n",
    "            return False\n",
    "        \n",
    "        print(\"Finding scene cuts... (this may take a moment)\")\n",
    "        scenes = self.find_scenes(video_path)\n",
    "        event_timestamps = sorted([e['timestamp'] for e in player_events])\n",
    "        \n",
    "        clips_to_extract = []\n",
    "        total_duration = 0.0\n",
    "        for ts in event_timestamps:\n",
    "            if total_duration >= self.max_duration: break\n",
    "            for start, end in scenes:\n",
    "                if start <= ts <= end:\n",
    "                    clip_duration = end - start\n",
    "                    if total_duration + clip_duration <= self.max_duration:\n",
    "                        if not any(c['start'] == start for c in clips_to_extract):\n",
    "                            clips_to_extract.append({'start': start, 'end': end})\n",
    "                            total_duration += clip_duration\n",
    "                    break\n",
    "        \n",
    "        if not clips_to_extract: \n",
    "            print(\"Could not map any events to scenes.\")\n",
    "            return False\n",
    "        \n",
    "        os.makedirs(self.temp_dir, exist_ok=True)\n",
    "        clip_paths = []\n",
    "        for i, clip in enumerate(tqdm(clips_to_extract, desc=\"Extracting scene clips\")):\n",
    "            clip_path = os.path.join(self.temp_dir, f\"clip_{i:04d}.mp4\")\n",
    "            command = ['ffmpeg', '-y', '-i', video_path, '-ss', str(clip['start']), '-to', str(clip['end']), '-c', 'copy', '-avoid_negative_ts', '1', clip_path]\n",
    "            result = subprocess.run(command, capture_output=True, text=True)\n",
    "            if result.returncode == 0: clip_paths.append(clip_path)\n",
    "        \n",
    "        concat_list_path = os.path.join(self.temp_dir, \"concat_list.txt\")\n",
    "        with open(concat_list_path, 'w') as f:\n",
    "            for path in clip_paths:\n",
    "                f.write(f\"file '{os.path.basename(path)}'\\n\")\n",
    "        \n",
    "        concat_command = ['ffmpeg', '-y', '-f', 'concat', '-safe', '0', '-i', concat_list_path, '-c', 'copy', output_path]\n",
    "        concat_result = subprocess.run(concat_command, cwd=self.temp_dir, capture_output=True, text=True)\n",
    "\n",
    "        if concat_result.returncode == 0:\n",
    "            print(f\"Highlight reel created: {output_path}\")\n",
    "            print(f\"Total duration: {total_duration:.2f} seconds\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"Failed to concatenate clips. FFmpeg error:\", concat_result.stderr)\n",
    "            return False\n",
    "\n",
    "if 'video_path' in locals() and video_path:\n",
    "    assembler = VideoAssembler()\n",
    "    highlight_path = f'/content/output/player_{target_player_id}_highlights.mp4'\n",
    "    success = assembler.assemble_highlight_reel(video_path, '/content/output/player_events.json', highlight_path)\n",
    "\n",
    "    if success:\n",
    "        print(f\"âœ… Highlight reel saved to: {highlight_path}\")\n",
    "    else:\n",
    "        print(\"âŒ Failed to create highlight reel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_results"
   },
   "outputs": [],
   "source": [
    "# 4. Download Results\n",
    "print(\"ðŸ“¥ Downloading Results\")\n",
    "\n",
    "from google.colab import files\n",
    "\n",
    "if 'success' in locals() and success and 'highlight_path' in locals() and os.path.exists(highlight_path):\n",
    "    files.download(highlight_path)\n",
    "    print(f\"âœ… Downloaded: {os.path.basename(highlight_path)}\")\n",
    "else:\n",
    "    print(f\"âŒ Could not download highlight reel. File not found or creation failed.\")\n",
    "\n",
    "if os.path.exists('/content/output/long_player_track.json'):\n",
    "    files.download('/content/output/long_player_track.json')\n",
    "    print(\"âœ… Downloaded: long_player_track.json\")\n",
    "\n",
    "if os.path.exists('/content/output/player_events.json'):\n",
    "    files.download('/content/output/player_events.json')\n",
    "    print(\"âœ… Downloaded: player_events.json\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ Pipeline complete! Check your browser's downloads folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "summary"
   },
   "outputs": [],
   "source": [
    "# 5. Pipeline Summary\n",
    "print(\"ðŸ“Š PIPELINE SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if 'video_path' in locals() and video_path and os.path.exists('/content/output/long_player_track.json') and os.path.exists('/content/output/player_events.json'):\n",
    "    with open('/content/output/long_player_track.json', 'r') as f:\n",
    "        long_tracks_summary = json.load(f)\n",
    "    with open('/content/output/player_events.json', 'r') as f:\n",
    "        player_events_summary = json.load(f)\n",
    "\n",
    "    print(f\"ðŸ“¹ Video processed: {os.path.basename(video_path)}\")\n",
    "    print(f\"ðŸŽ¯ Target player ID: {target_player_id}\")\n",
    "    print(f\"ðŸ–¼ï¸ Total frames processed: {len(long_tracks_summary)}\")\n",
    "    print(f\"âœ¨ Events detected for player: {len(player_events_summary)}\")\n",
    "\n",
    "    if 'success' in locals() and success and 'highlight_path' in locals() and os.path.exists(highlight_path):\n",
    "        print(f\"â±ï¸ Highlight reel generated successfully.\")\n",
    "        file_size = os.path.getsize(highlight_path) / (1024 * 1024)\n",
    "        print(f\"ðŸ“ Output file size: {file_size:.1f} MB\")\n",
    "    else:\n",
    "        print(\"â±ï¸ Highlight reel was not generated.\")\n",
    "else:\n",
    "    print(\"Could not generate summary. One or more output files are missing.\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"ðŸŽ‰ Pipeline finished!\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
"