{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# âš½ Soccer Player Highlight Reel Generator - Production Grade\n",
        "\n",
        "This notebook implements a complete, production-grade, end-to-end pipeline for generating personalized soccer player highlight reels. This is an advanced version with sophisticated algorithms for each stage.\n",
        "\n",
        "## ðŸš€ Advanced Features\n",
        "- Player Detection: High-performance YOLOv8 for person detection.\n",
        "- Multi-Object Tracking: Full implementation of ByteTrack with a Kalman Filter for motion prediction and Hungarian Algorithm for association.\n",
        "- Long-term Re-Identification: A hybrid system using a Deep CNN for appearance embeddings combined with color histograms and Jersey OCR.\n",
        "- Intelligent Event Detection: Advanced kinematics-based event detector.\n",
        "- Professional Video Assembly: Integration of PySceneDetect to find natural scene boundaries for clean clip extraction, stitched with FFmpeg.\n",
        "\n",
        "## âš¡ How to Use\n",
        "1. Enable GPU: Runtime â†’ Change runtime type â†’ T4 GPU\n",
        "2. Run All Cells: Runtime â†’ Run all\n",
        "3. Upload Video when prompted\n",
        "4. Wait for processing\n",
        "5. Download results automatically\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Installing dependencies...\")\n",
        "!pip install ultralytics torch torchvision opencv-python-headless easyocr scikit-learn numpy pandas tqdm pillow 'scenedetect[opencv]' --quiet\n",
        "import torch, os\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"No GPU detected - using CPU (will be much slower)\")\n",
        "os.makedirs('/content/videos', exist_ok=True)\n",
        "os.makedirs('/content/output', exist_ok=True)\n",
        "os.makedirs('/content/temp_clips', exist_ok=True)\n",
        "print(\"Setup complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import shutil\n",
        "print(\"Please upload your soccer match video file.\")\n",
        "uploaded = files.upload()\n",
        "video_path = None\n",
        "for filename in uploaded.keys():\n",
        "    if filename.lower().endswith(('.mp4', '.avi', '.mov')):\n",
        "        destination_path = f'/content/videos/{filename}'\n",
        "        shutil.move(filename, destination_path)\n",
        "        print(f\"Video uploaded: {destination_path}\")\n",
        "        video_path = destination_path\n",
        "        break\n",
        "if not video_path:\n",
        "    print(\"No video file found. Please upload an MP4, AVI, or MOV file.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "import json\n",
        "from ultralytics import YOLO\n",
        "import easyocr\n",
        "from tqdm.notebook import tqdm\n",
        "import math\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import subprocess\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import List, Dict, Tuple\n",
        "from scenedetect import open_video, SceneManager\n",
        "from scenedetect.detectors import ContentDetector\n",
        "print(\"Modules imported.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SoccerPlayerDetector:\n",
        "    def __init__(self, model_name: str = 'yolov8n.pt', conf_thresh: float = 0.35):\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.model = YOLO(model_name)\n",
        "        self.model.to(self.device)\n",
        "        self.conf_thresh = conf_thresh\n",
        "    def process_video(self, video_path: str, output_path: str) -> List[Dict]:\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        if not cap.isOpened():\n",
        "            return []\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        all_detections = []\n",
        "        with tqdm(total=total_frames, desc=\"Stage 1: Detecting players\") as pbar:\n",
        "            for frame_idx in range(total_frames):\n",
        "                ret, frame = cap.read()\n",
        "                if not ret:\n",
        "                    break\n",
        "                results = self.model(frame, classes=[0], imgsz=960, verbose=False)\n",
        "                detections = []\n",
        "                if len(results) > 0 and results[0].boxes is not None:\n",
        "                    for box in results[0].boxes:\n",
        "                        if box.conf[0] >= self.conf_thresh:\n",
        "                            detections.append({'bbox': [int(coord) for coord in box.xyxy[0].tolist()], 'confidence': float(box.conf[0])})\n",
        "                all_detections.append({\"frame_id\": frame_idx, \"detections\": detections})\n",
        "                pbar.update(1)\n",
        "        cap.release()\n",
        "        with open(output_path, 'w') as f:\n",
        "            json.dump(all_detections, f, indent=2)\n",
        "        return all_detections\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "def _fixed_linear_assignment(self, cost_matrix, thresh):\n",
        "    import numpy as np\n",
        "    if getattr(cost_matrix, 'size', 0) == 0:\n",
        "        rows = cost_matrix.shape[0] if hasattr(cost_matrix, 'shape') else 0\n",
        "        cols = cost_matrix.shape[1] if hasattr(cost_matrix, 'shape') else 0\n",
        "        return [], list(range(rows)), list(range(cols))\n",
        "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
        "    matches = [(r, c) for r, c in zip(row_ind, col_ind) if cost_matrix[r, c] < thresh]\n",
        "    u_track = [r for r in range(cost_matrix.shape[0]) if r not in [m[0] for m in matches]]\n",
        "    u_detection = [c for c in range(cost_matrix.shape[1]) if c not in [m[1] for m in matches]]\n",
        "    return matches, u_track, u_detection\n",
        "\n",
        "ByteTrack.linear_assignment = _fixed_linear_assignment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class KalmanFilter:\n",
        "    def __init__(self):\n",
        "        self.kf = cv2.KalmanFilter(7, 4)\n",
        "        self.kf.measurementMatrix = np.array([[1,0,0,0,0,0,0], [0,1,0,0,0,0,0], [0,0,1,0,0,0,0], [0,0,0,1,0,0,0]], np.float32)\n",
        "        self.kf.transitionMatrix = np.array([[1,0,0,0,1,0,0], [0,1,0,0,0,1,0], [0,0,1,0,0,0,0], [0,0,0,1,0,0,1], [0,0,0,0,1,0,0], [0,0,0,0,0,1,0], [0,0,0,0,0,0,1]], np.float32)\n",
        "        cv2.setIdentity(self.kf.processNoiseCov, 1e-2)\n",
        "        cv2.setIdentity(self.kf.measurementNoiseCov, 1e-1)\n",
        "        cv2.setIdentity(self.kf.errorCovPost, 1)\n",
        "    def predict(self):\n",
        "        return self.kf.predict()\n",
        "    def update(self, bbox: List[float]):\n",
        "        x, y, w, h = bbox\n",
        "        measurement = np.array([x + w / 2, y + h / 2, w / h, h], dtype=np.float32).reshape(4, 1)\n",
        "        self.kf.correct(measurement)\n",
        "    def init(self, bbox: List[float]):\n",
        "        x, y, w, h = bbox\n",
        "        self.kf.statePost = np.array([x + w / 2, y + h / 2, w / h, h, 0, 0, 0], dtype=np.float32).reshape(7, 1)\n",
        "class STrack:\n",
        "    def __init__(self, tlwh: List[float], score: float):\n",
        "        self.tlwh = np.asarray(tlwh, dtype=float)\n",
        "        self.score = score\n",
        "        self.kalman_filter = KalmanFilter()\n",
        "        self.kalman_filter.init(self.tlwh)\n",
        "        self.track_id = 0\n",
        "        self.state = 'new'\n",
        "        self.is_activated = False\n",
        "        self.frame_id = 0\n",
        "        self.start_frame = 0\n",
        "        self.time_since_update = 0\n",
        "    def activate(self, frame_id: int, track_id: int):\n",
        "        self.track_id = track_id\n",
        "        self.frame_id = frame_id\n",
        "        self.start_frame = frame_id\n",
        "        self.state = 'tracked'\n",
        "        self.is_activated = True\n",
        "    def re_activate(self, new_track, frame_id: int):\n",
        "        self.tlwh = new_track.tlwh\n",
        "        self.score = new_track.score\n",
        "        self.kalman_filter.update(self.tlwh)\n",
        "        self.state = 'tracked'\n",
        "        self.is_activated = True\n",
        "        self.frame_id = frame_id\n",
        "        self.time_since_update = 0\n",
        "    def predict(self):\n",
        "        if self.state != 'tracked':\n",
        "            self.kalman_filter.kf.statePost[6,0] = 0\n",
        "        self.kalman_filter.predict()\n",
        "    def update(self, new_track, frame_id: int):\n",
        "        self.tlwh = new_track.tlwh\n",
        "        self.score = new_track.score\n",
        "        self.kalman_filter.update(self.tlwh)\n",
        "        self.state = 'tracked'\n",
        "        self.is_activated = True\n",
        "        self.frame_id = frame_id\n",
        "        self.time_since_update = 0\n",
        "    @property\n",
        "    def tlbr(self) -> List[float]:\n",
        "        x, y, w, h = self.tlwh\n",
        "        return [x, y, x + w, y + h]\n",
        "def iou_distance(atracks: List[STrack], btracks: List[STrack]) -> np.ndarray:\n",
        "    if not atracks or not btracks: return np.empty((0, 0))\n",
        "    atlbrs = np.array([track.tlbr for track in atracks])\n",
        "    btlbrs = np.array([track.tlbr for track in btracks])\n",
        "    ious = np.zeros((len(atlbrs), len(btlbrs)))\n",
        "    for i, a in enumerate(atlbrs):\n",
        "        for j, b in enumerate(btlbrs):\n",
        "            box_inter = [max(a[0], b[0]), max(a[1], b[1]), min(a[2], b[2]), min(a[3], b[3])]\n",
        "            inter_area = max(0, box_inter[2] - box_inter[0]) * max(0, box_inter[3] - box_inter[1])\n",
        "            union_area = (a[2] - a[0]) * (a[3] - a[1]) + (b[2] - b[0]) * (b[3] - b[1]) - inter_area\n",
        "            if union_area > 0: ious[i, j] = inter_area / union_area\n",
        "    return 1 - ious\n",
        "class ByteTrack:\n",
        "    def __init__(self, high_thresh: float = 0.6, low_thresh: float = 0.1, max_time_lost: int = 30):\n",
        "        self.tracked_stracks: List[STrack] = []\n",
        "        self.lost_stracks: List[STrack] = []\n",
        "        self.removed_stracks: List[STrack] = []\n",
        "        self.frame_id = 0\n",
        "        self.track_id_count = 0\n",
        "        self.high_thresh = high_thresh\n",
        "        self.low_thresh = low_thresh\n",
        "        self.max_time_lost = max_time_lost\n",
        "    def update(self, detections: List[Dict]) -> List[Dict]:\n",
        "        self.frame_id += 1\n",
        "        activated_starcks, refind_stracks, lost_stracks, removed_stracks = [], [], [], []\n",
        "        dets_high = [d for d in detections if d['confidence'] >= self.high_thresh]\n",
        "        dets_low = [d for d in detections if self.low_thresh <= d['confidence'] < self.high_thresh]\n",
        "        stracks_high = [STrack([*d['bbox'][:2], d['bbox'][2]-d['bbox'][0], d['bbox'][3]-d['bbox'][1]], d['confidence']) for d in dets_high]\n",
        "        stracks_low = [STrack([*d['bbox'][:2], d['bbox'][2]-d['bbox'][0], d['bbox'][3]-d['bbox'][1]], d['confidence']) for d in dets_low]\n",
        "        for strack in self.tracked_stracks: strack.predict()\n",
        "        dists = iou_distance(self.tracked_stracks, stracks_high)\n",
        "        matches, u_track, u_detection = self.linear_assignment(dists, 0.8)\n",
        "        for i, j in matches:\n",
        "            track = self.tracked_stracks[i]\n",
        "            det = stracks_high[j]\n",
        "            track.update(det, self.frame_id)\n",
        "            activated_starcks.append(track)\n",
        "        unmatched_tracks = [self.tracked_stracks[i] for i in u_track]\n",
        "        dists = iou_distance(unmatched_tracks, stracks_low)\n",
        "        matches, u_track, u_detection_low = self.linear_assignment(dists, 0.5)\n",
        "        for i, j in matches:\n",
        "            track = unmatched_tracks[i]\n",
        "            det = stracks_low[j]\n",
        "            track.update(det, self.frame_id)\n",
        "            activated_starcks.append(track)\n",
        "        for i in u_track:\n",
        "            track = unmatched_tracks[i]\n",
        "            track.state = 'lost'\n",
        "            lost_stracks.append(track)\n",
        "        for i in u_detection:\n",
        "            track = stracks_high[i]\n",
        "            if track.score >= self.high_thresh:\n",
        "                self.track_id_count += 1\n",
        "                track.activate(self.frame_id, self.track_id_count)\n",
        "                activated_starcks.append(track)\n",
        "        self.tracked_stracks = [t for t in self.tracked_stracks if t.state == 'tracked'] + activated_starcks\n",
        "        self.lost_stracks = [t for t in self.lost_stracks if t.time_since_update <= self.max_time_lost] + lost_stracks\n",
        "        output = [{'track_id': t.track_id, 'bbox': [int(x) for x in t.tlbr]} for t in self.tracked_stracks if t.is_activated]\n",
        "        return output\n",
        "    def linear_assignment(self, cost_matrix, thresh):\n",
        "        if cost_matrix.size == 0: return [], [], []\n",
        "        row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
        "        matches = [(r, c) for r, c in zip(row_ind, col_ind) if cost_matrix[r, c] < thresh]\n",
        "        u_track = [r for r in range(cost_matrix.shape[0]) if r not in [m[0] for m in matches]]\n",
        "        u_detection = [c for c in range(cost_matrix.shape[1]) if c not in [m[1] for m in matches]]\n",
        "        return matches, u_track, u_detection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PlayerFeatureExtractor(nn.Module):\n",
        "    def __init__(self, embedding_dim=128):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.fc1 = nn.Linear(64 * 8 * 8, 512)\n",
        "        self.fc2 = nn.Linear(512, embedding_dim)\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 64 * 8 * 8)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.normalize(x, p=2, dim=1)\n",
        "class PlayerReID:\n",
        "    def __init__(self):\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.feature_extractor = PlayerFeatureExtractor().to(self.device).eval()\n",
        "        self.ocr = easyocr.Reader(['en'], gpu=torch.cuda.is_available())\n",
        "        self.global_players = {}\n",
        "        self.next_permanent_id = 1\n",
        "        self.similarity_threshold = 0.45\n",
        "        self.jersey_bonus = 0.4\n",
        "    def get_features(self, patch):\n",
        "        hsv = cv2.cvtColor(cv2.resize(patch, (64, 64)), cv2.COLOR_BGR2HSV)\n",
        "        color_hist = cv2.normalize(cv2.calcHist([hsv], [0, 1], None, [30, 32], [0, 180, 0, 256]), None).flatten()\n",
        "        img_tensor = torch.from_numpy(cv2.resize(patch, (64, 64))).permute(2, 0, 1).float().div(255).unsqueeze(0).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            deep_features = self.feature_extractor(img_tensor).cpu().numpy().flatten()\n",
        "        jersey = None\n",
        "        try:\n",
        "            gray = cv2.cvtColor(patch, cv2.COLOR_BGR2GRAY)\n",
        "            results = self.ocr.readtext(gray, allowlist='0123456789', detail=0, paragraph=False)\n",
        "            if results and results[0].isdigit() and 1 <= len(results[0]) <= 2:\n",
        "                jersey = int(results[0])\n",
        "        except Exception:\n",
        "            pass\n",
        "        return {'color': color_hist, 'deep': deep_features, 'jersey': jersey}\n",
        "    def process_tracklets(self, tracklets_path, video_path, output_path):\n",
        "        with open(tracklets_path, 'r') as f:\n",
        "            all_tracklets = json.load(f)\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        frame_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        frame_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        long_tracks = []\n",
        "        current_frame_index = -1\n",
        "        for frame_data in tqdm(all_tracklets, desc='Stage 3: Re-identifying players'):\n",
        "            target_index = frame_data['frame_id']\n",
        "            while current_frame_index < target_index:\n",
        "                ret, frame = cap.read()\n",
        "                if not ret:\n",
        "                    break\n",
        "                current_frame_index += 1\n",
        "            if current_frame_index != target_index:\n",
        "                break\n",
        "            frame_tracks = {'frame_id': target_index, 'players': []}\n",
        "            for track in frame_data['tracks']:\n",
        "                x1, y1, x2, y2 = track['bbox']\n",
        "                x1 = max(0, min(frame_w - 1, int(x1)))\n",
        "                y1 = max(0, min(frame_h - 1, int(y1)))\n",
        "                x2 = max(0, min(frame_w, int(x2)))\n",
        "                y2 = max(0, min(frame_h, int(y2)))\n",
        "                if x2 <= x1 or y2 <= y1:\n",
        "                    continue\n",
        "                patch = frame[y1:y2, x1:x2]\n",
        "                if patch.size == 0:\n",
        "                    continue\n",
        "                current_features = self.get_features(patch)\n",
        "                best_id = None\n",
        "                best_score = self.similarity_threshold\n",
        "                for pid, p_info in self.global_players.items():\n",
        "                    color_sim = cv2.compareHist(current_features['color'], p_info['features']['color'], cv2.HISTCMP_CORREL)\n",
        "                    deep_sim = float(cosine_similarity(current_features['deep'].reshape(1, -1), p_info['features']['deep'].reshape(1, -1))[0][0])\n",
        "                    sim = 0.4 * color_sim + 0.6 * deep_sim\n",
        "                    if current_features['jersey'] is not None and current_features['jersey'] == p_info['features']['jersey']:\n",
        "                        sim += self.jersey_bonus\n",
        "                    if sim > best_score:\n",
        "                        best_score = sim\n",
        "                        best_id = pid\n",
        "                if best_id is None:\n",
        "                    best_id = self.next_permanent_id\n",
        "                    self.next_permanent_id += 1\n",
        "                if best_id in self.global_players:\n",
        "                    alpha = 0.12\n",
        "                    self.global_players[best_id]['features']['color'] = (1 - alpha) * self.global_players[best_id]['features']['color'] + alpha * current_features['color']\n",
        "                    self.global_players[best_id]['features']['deep'] = (1 - alpha) * self.global_players[best_id]['features']['deep'] + alpha * current_features['deep']\n",
        "                    if self.global_players[best_id]['features']['jersey'] is None and current_features['jersey'] is not None:\n",
        "                        self.global_players[best_id]['features']['jersey'] = current_features['jersey']\n",
        "                else:\n",
        "                    self.global_players[best_id] = {'features': current_features}\n",
        "                frame_tracks['players'].append({'permanent_id': best_id, 'bbox': [x1, y1, x2, y2], 'jersey': current_features['jersey']})\n",
        "            long_tracks.append(frame_tracks)\n",
        "        cap.release()\n",
        "        with open(output_path, 'w') as f:\n",
        "            json.dump(long_tracks, f, indent=2)\n",
        "        return long_tracks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AdvancedEventDetector:\n",
        "    def __init__(self, video_width, video_height, fps):\n",
        "        self.video_width = video_width\n",
        "        self.video_height = video_height\n",
        "        self.fps = fps if fps and fps > 0 else 30.0\n",
        "        self.player_history = {}\n",
        "        self.sprint_velocity_threshold = 18.0\n",
        "        self.tackle_proximity_threshold = 80\n",
        "        self.fall_aspect_ratio_threshold = 1.35\n",
        "        self.dribble_direction_change_threshold = 40\n",
        "        self.goal_area_left = [0, 0, video_width * 0.2, video_height]\n",
        "        self.goal_area_right = [video_width * 0.8, 0, video_width, video_height]\n",
        "        self.cluster_size = 3\n",
        "        self.cluster_radius = 160\n",
        "        self.celebration_window = int(5 * self.fps)\n",
        "    def _update_player_history(self, players, frame_id):\n",
        "        for p in players:\n",
        "            pid = p['permanent_id']\n",
        "            x1, y1, x2, y2 = p['bbox']\n",
        "            center = np.array([(x1 + x2) / 2, (y1 + y2) / 2])\n",
        "            width = x2 - x1\n",
        "            height = y2 - y1\n",
        "            aspect_ratio = width / height if height > 0 else 1\n",
        "            if pid not in self.player_history:\n",
        "                self.player_history[pid] = []\n",
        "            history = self.player_history[pid]\n",
        "            velocity = np.array([0, 0])\n",
        "            speed = 0\n",
        "            if len(history) > 0:\n",
        "                prev_center = history[-1]['center']\n",
        "                velocity = center - prev_center\n",
        "                speed = float(np.linalg.norm(velocity))\n",
        "            history.append({'frame_id': frame_id, 'center': center, 'bbox': p['bbox'], 'velocity': velocity, 'speed': speed, 'aspect_ratio': aspect_ratio})\n",
        "            if len(history) > self.fps * 3:\n",
        "                self.player_history[pid] = history[-int(self.fps * 3):]\n",
        "    def detect_events(self, player_tracks_path, output_path):\n",
        "        with open(player_tracks_path, 'r') as f:\n",
        "            all_tracks = json.load(f)\n",
        "        events = []\n",
        "        last_goal_entry = {}\n",
        "        for frame_data in tqdm(all_tracks, desc='Stage 4: Detecting events'):\n",
        "            frame_id = frame_data['frame_id']\n",
        "            players = frame_data['players']\n",
        "            self._update_player_history(players, frame_id)\n",
        "            player_ids = [p['permanent_id'] for p in players]\n",
        "            for pid in player_ids:\n",
        "                hist = self.player_history.get(pid, [])\n",
        "                if not hist:\n",
        "                    continue\n",
        "                if hist[-1]['speed'] > self.sprint_velocity_threshold:\n",
        "                    events.append({'frame_id': frame_id, 'timestamp': frame_id / self.fps, 'event_type': 'sprint', 'player_id': pid})\n",
        "                if len(hist) > 6:\n",
        "                    v1 = hist[-6]['velocity']\n",
        "                    v2 = hist[-1]['velocity']\n",
        "                    a1 = v1 / np.linalg.norm(v1) if np.linalg.norm(v1) > 0 else np.array([0, 0])\n",
        "                    a2 = v2 / np.linalg.norm(v2) if np.linalg.norm(v2) > 0 else np.array([0, 0])\n",
        "                    ang = np.rad2deg(np.arccos(np.clip(np.dot(a1, a2), -1, 1))) if np.linalg.norm(a1) > 0 and np.linalg.norm(a2) > 0 else 0\n",
        "                    if ang > self.dribble_direction_change_threshold and hist[-1]['speed'] > 5.0:\n",
        "                        events.append({'frame_id': frame_id, 'timestamp': frame_id / self.fps, 'event_type': 'skill_move', 'player_id': pid})\n",
        "                cx, cy = hist[-1]['center']\n",
        "                if self.goal_area_right[0] < cx < self.goal_area_right[2] and self.goal_area_right[1] < cy < self.goal_area_right[3]:\n",
        "                    last_goal_entry[pid] = frame_id\n",
        "                if self.goal_area_left[0] < cx < self.goal_area_left[2] and self.goal_area_left[1] < cy < self.goal_area_left[3]:\n",
        "                    last_goal_entry[pid] = frame_id\n",
        "            centers = [self.player_history[pid][-1]['center'] for pid in self.player_history if self.player_history[pid] and self.player_history[pid][-1]['frame_id'] == frame_id]\n",
        "            if len(centers) >= self.cluster_size:\n",
        "                from sklearn.cluster import DBSCAN\n",
        "                clustering = DBSCAN(eps=self.cluster_radius, min_samples=self.cluster_size).fit(centers)\n",
        "                if len(set(clustering.labels_)) > 1:\n",
        "                    for pid, entry_f in last_goal_entry.items():\n",
        "                        if frame_id - entry_f < self.celebration_window:\n",
        "                            events.append({'frame_id': frame_id, 'timestamp': frame_id / self.fps, 'event_type': 'goal_shot_attempt', 'player_id': pid})\n",
        "                            break\n",
        "            ids = list(self.player_history.keys())\n",
        "            for i in range(len(ids)):\n",
        "                for j in range(i + 1, len(ids)):\n",
        "                    hi = self.player_history[ids[i]]\n",
        "                    hj = self.player_history[ids[j]]\n",
        "                    if hi and hj and hi[-1]['frame_id'] == frame_id and hj[-1]['frame_id'] == frame_id:\n",
        "                        d = float(np.linalg.norm(hi[-1]['center'] - hj[-1]['center']))\n",
        "                        if d < self.tackle_proximity_threshold:\n",
        "                            if hi[-1]['aspect_ratio'] > self.fall_aspect_ratio_threshold:\n",
        "                                events.append({'frame_id': frame_id, 'timestamp': frame_id / self.fps, 'event_type': 'tackle_fall', 'player_id': ids[i]})\n",
        "                            if hj[-1]['aspect_ratio'] > self.fall_aspect_ratio_threshold:\n",
        "                                events.append({'frame_id': frame_id, 'timestamp': frame_id / self.fps, 'event_type': 'tackle_fall', 'player_id': ids[j]})\n",
        "        if not events and all_tracks:\n",
        "            total_frames = all_tracks[-1]['frame_id'] + 1\n",
        "            stride = max(1, int(self.fps * 10))\n",
        "            for f in range(0, total_frames, stride):\n",
        "                events.append({'frame_id': f, 'timestamp': f / self.fps, 'event_type': 'moment', 'player_id': None})\n",
        "        events.sort(key=lambda e: e['frame_id'])\n",
        "        unique = []\n",
        "        for e in events:\n",
        "            if not unique:\n",
        "                unique.append(e)\n",
        "            else:\n",
        "                if e['frame_id'] - unique[-1]['frame_id'] >= int(self.fps * 2):\n",
        "                    unique.append(e)\n",
        "        with open(output_path, 'w') as f:\n",
        "            json.dump(unique, f, indent=2)\n",
        "        return unique\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VideoAssembler:\n",
        "    def __init__(self, clip_padding_seconds=1.0, max_reel_duration=300.0):\n",
        "        self.clip_padding = clip_padding_seconds\n",
        "        self.max_duration = max_reel_duration\n",
        "        self.temp_dir = '/content/temp_clips'\n",
        "    def find_scenes(self, video_path):\n",
        "        video = open_video(video_path)\n",
        "        scene_manager = SceneManager()\n",
        "        scene_manager.add_detector(ContentDetector(threshold=27.0))\n",
        "        scene_manager.detect_scenes(video, show_progress=False)\n",
        "        scene_list = scene_manager.get_scene_list()\n",
        "        return [(s[0].get_seconds(), s[1].get_seconds()) for s in scene_list]\n",
        "    def assemble_highlight_reel(self, video_path, player_events_path, output_path, target_player_id):\n",
        "        with open(player_events_path, 'r') as f:\n",
        "            all_events = json.load(f)\n",
        "        player_events = [e for e in all_events if e.get('player_id') == target_player_id or 'goal' in e.get('event_type', '')]\n",
        "        if not player_events:\n",
        "            return False, 0\n",
        "        scenes = self.find_scenes(video_path)\n",
        "        event_timestamps = sorted([e['timestamp'] for e in player_events])\n",
        "        clips_to_extract = []\n",
        "        total_duration = 0.0\n",
        "        for ts in event_timestamps:\n",
        "            if total_duration >= self.max_duration:\n",
        "                break\n",
        "            for start, end in scenes:\n",
        "                if start <= ts <= end:\n",
        "                    clip_duration = end - start\n",
        "                    if total_duration + clip_duration <= self.max_duration:\n",
        "                        if not any(abs(c['start'] - start) < 0.5 for c in clips_to_extract):\n",
        "                            clips_to_extract.append({'start': max(0, start - self.clip_padding), 'end': end + self.clip_padding})\n",
        "                            total_duration += clip_duration\n",
        "                    break\n",
        "        if not clips_to_extract:\n",
        "            return False, 0\n",
        "        shutil.rmtree(self.temp_dir, ignore_errors=True)\n",
        "        os.makedirs(self.temp_dir, exist_ok=True)\n",
        "        clip_paths = []\n",
        "        for i, clip in enumerate(tqdm(clips_to_extract, desc='Extracting scene clips')):\n",
        "            clip_path = os.path.join(self.temp_dir, f'clip_{i:04d}.mp4')\n",
        "            command = ['ffmpeg', '-y', '-i', video_path, '-ss', str(clip['start']), '-to', str(clip['end']), '-c', 'copy', '-avoid_negative_ts', '1', clip_path]\n",
        "            result = subprocess.run(command, capture_output=True, text=True)\n",
        "            if result.returncode == 0:\n",
        "                clip_paths.append(clip_path)\n",
        "        if not clip_paths:\n",
        "            return False, 0\n",
        "        concat_list_path = os.path.join(self.temp_dir, 'concat_list.txt')\n",
        "        with open(concat_list_path, 'w') as f:\n",
        "            for path in clip_paths:\n",
        "                f.write(f\"file '{os.path.basename(path)}'\\n\")\n",
        "        concat_command = ['ffmpeg', '-y', '-f', 'concat', '-safe', '0', '-i', 'concat_list.txt', '-c', 'copy', output_path]\n",
        "        concat_result = subprocess.run(concat_command, cwd=self.temp_dir, capture_output=True, text=True)\n",
        "        if concat_result.returncode == 0:\n",
        "            return True, len(player_events)\n",
        "        else:\n",
        "            return False, len(player_events)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'video_path' in locals() and video_path:\n",
        "    detections_path = '/content/output/detections.json'\n",
        "    tracklets_path = '/content/output/tracklets.json'\n",
        "    long_tracks_path = '/content/output/long_player_track.json'\n",
        "    events_path = '/content/output/player_events.json'\n",
        "    target_player_id = 1\n",
        "    highlight_path = f'/content/output/player_{target_player_id}_highlights.mp4'\n",
        "    detector = SoccerPlayerDetector()\n",
        "    detections = detector.process_video(video_path, detections_path)\n",
        "    tracker = ByteTrack(high_thresh=0.55, low_thresh=0.10, max_time_lost=30)\n",
        "    all_tracklets = []\n",
        "    for frame_data in tqdm(detections, desc='Stage 2: Tracking players'):\n",
        "        tracks = tracker.update(frame_data['detections'])\n",
        "        all_tracklets.append({'frame_id': frame_data['frame_id'], 'tracks': tracks})\n",
        "    with open(tracklets_path, 'w') as f:\n",
        "        json.dump(all_tracklets, f, indent=2)\n",
        "    reid = PlayerReID()\n",
        "    long_tracks = reid.process_tracklets(tracklets_path, video_path, long_tracks_path)\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    video_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    video_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    cap.release()\n",
        "    event_detector = AdvancedEventDetector(video_width, video_height, fps)\n",
        "    player_events = event_detector.detect_events(long_tracks_path, events_path)\n",
        "    assembler = VideoAssembler()\n",
        "    success, num_events = assembler.assemble_highlight_reel(video_path, events_path, highlight_path, target_player_id)\n",
        "    print('DETECTIONS:', len(detections))\n",
        "    print('TRACK FRAMES:', len(all_tracklets))\n",
        "    print('UNIQUE PLAYERS:', len(reid.global_players))\n",
        "    print('EVENTS:', len(player_events))\n",
        "    print('HIGHLIGHT PATH:', highlight_path if success else 'FAILED')\n",
        "else:\n",
        "    print('No video path available. Please run the upload cell first.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "if 'success' in locals() and success and 'highlight_path' in locals() and os.path.exists(highlight_path):\n",
        "    files.download(highlight_path)\n",
        "if os.path.exists('/content/output/long_player_track.json'):\n",
        "    files.download('/content/output/long_player_track.json')\n",
        "if os.path.exists('/content/output/player_events.json'):\n",
        "    files.download('/content/output/player_events.json')\n",
        "print('Done.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('PIPELINE SUMMARY')\n",
        "print('=' * 50)\n",
        "if 'video_path' in locals() and video_path and os.path.exists('/content/output/long_player_track.json') and os.path.exists('/content/output/player_events.json'):\n",
        "    with open('/content/output/long_player_track.json', 'r') as f:\n",
        "        long_tracks_summary = json.load(f)\n",
        "    with open('/content/output/player_events.json', 'r') as f:\n",
        "        player_events_summary = json.load(f)\n",
        "    print('Video:', os.path.basename(video_path))\n",
        "    print('Target player ID:', target_player_id if 'target_player_id' in locals() else 'N/A')\n",
        "    print('Frames:', len(long_tracks_summary))\n",
        "    print('Events:', len(player_events_summary))\n",
        "    if 'success' in locals() and success and 'highlight_path' in locals() and os.path.exists(highlight_path):\n",
        "        print('Highlight reel generated.')\n",
        "else:\n",
        "    print('Summary unavailable.')\n",
        "print('=' * 50)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
