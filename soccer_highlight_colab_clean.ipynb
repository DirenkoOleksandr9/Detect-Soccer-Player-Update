{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# ⚽ Soccer Player Highlight Reel Generator - Production Grade\n",
    "\n",
    "This notebook implements a complete, production-grade, end-to-end pipeline for generating personalized soccer player highlight reels. This is an advanced version with sophisticated algorithms for each stage.\n",
    "\n",
    "## 🚀 Advanced Features\n",
    "- **Player Detection**: High-performance YOLOv8 for person detection.\n",
    "- **Multi-Object Tracking**: Full implementation of **ByteTrack** with a **Kalman Filter** for motion prediction and Hungarian Algorithm for association.\n",
    "- **Long-term Re-Identification**: A hybrid system using a **Deep CNN** for appearance embeddings combined with color histograms and Jersey OCR.\n",
    "- **Intelligent Event Detection**: A sophisticated heuristic model that analyzes player velocity, goal proximity, and clustering to identify key moments.\n",
    "- **Professional Video Assembly**: Integration of **PySceneDetect** to find natural scene boundaries for clean clip extraction, stitched with FFmpeg.\n",
    "\n",
    "## ⚡ How to Use\n",
    "1. **Enable GPU**: Go to `Runtime` → `Change runtime type` and select `T4 GPU`.\n",
    "2. **Run All Cells**: Click `Runtime` → `Run all`.\n",
    "3. **Upload Video**: An upload prompt will appear. Select your soccer match video.\n",
    "4. **Wait**: The pipeline will process the video. This is a computationally intensive process.\n",
    "5. **Download**: The final highlight reel and data files will be automatically downloaded."
   ],
   "metadata": {
    "id": "header"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 🔧 1. Setup & Installation"
   ],
   "metadata": {
    "id": "setup_markdown"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"Installing dependencies...\")\n",
    "!pip install ultralytics torch torchvision opencv-python-headless easyocr scikit-learn numpy pandas tqdm pillow 'scenedetect[opencv]' --quiet\n",
    "\n",
    "import torch\n",
    "import os\n",
    "\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"⚠️ No GPU detected - using CPU (will be much slower)\")\n",
    "\n",
    "os.makedirs('/content/videos', exist_ok=True)\n",
    "os.makedirs('/content/output', exist_ok=True)\n",
    "os.makedirs('/content/temp_clips', exist_ok=True)\n",
    "\n",
    "print(\"✅ Setup complete!\")"
   ],
   "metadata": {
    "id": "install_deps"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 📁 2. Upload Your Video"
   ],
   "metadata": {
    "id": "upload_markdown"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import files\n",
    "import shutil\n",
    "\n",
    "print(\"Please upload your soccer match video file.\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "video_path = None\n",
    "for filename in uploaded.keys():\n",
    "    if filename.lower().endswith(('.mp4', '.avi', '.mov')):\n",
    "        destination_path = f'/content/videos/{filename}'\n",
    "        shutil.move(filename, destination_path)\n",
    "        print(f\"✅ Video uploaded: {destination_path}\")\n",
    "        video_path = destination_path\n",
    "        break\n",
    "\n",
    "if not video_path:\n",
    "    print(\"❌ No video file found. Please upload an MP4, AVI, or MOV file.\")"
   ],
   "metadata": {
    "id": "upload_video"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 🚀 3. Run the Full End-to-End Pipeline"
   ],
   "metadata": {
    "id": "pipeline_header"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "from ultralytics import YOLO\n",
    "import easyocr\n",
    "from tqdm.notebook import tqdm\n",
    "import math\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import subprocess\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Dict, Tuple\n",
    "from scenedetect import open_video, SceneManager\n",
    "from scenedetect.detectors import ContentDetector\n",
    "\n",
    "print(\"✅ All modules imported successfully!\")"
   ],
   "metadata": {
    "id": "import_modules"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class SoccerPlayerDetector:\n",
    "    \"\"\"Detects players in a video using YOLOv8.\"\"\"\n",
    "    def __init__(self, model_name: str = 'yolov8n.pt', conf_thresh: float = 0.4):\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model = YOLO(model_name)\n",
    "        self.model.to(self.device)\n",
    "        self.conf_thresh = conf_thresh\n",
    "        print(f\"Detector initialized on {self.device} with model {model_name}\")\n",
    "\n",
    "    def process_video(self, video_path: str, output_path: str) -> List[Dict]:\n",
    "        \"\"\"Processes a video file to detect players in each frame.\"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            print(f\"Error: Could not open video {video_path}\")\n",
    "            return []\n",
    "        \n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        all_detections = []\n",
    "        \n",
    "        with tqdm(total=total_frames, desc=\"Stage 1: Detecting players\") as pbar:\n",
    "            for frame_idx in range(total_frames):\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                \n",
    "                results = self.model(frame, classes=[0], verbose=False)\n",
    "                \n",
    "                detections = []\n",
    "                if len(results) > 0 and results[0].boxes is not None:\n",
    "                    for box in results[0].boxes:\n",
    "                        if box.conf[0] >= self.conf_thresh:\n",
    "                            detections.append({\n",
    "                                'bbox': [int(coord) for coord in box.xyxy[0].tolist()],\n",
    "                                'confidence': float(box.conf[0])\n",
    "                            })\n",
    "                \n",
    "                all_detections.append({\"frame_id\": frame_idx, \"detections\": detections})\n",
    "                pbar.update(1)\n",
    "        \n",
    "        cap.release()\n",
    "        \n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(all_detections, f, indent=2)\n",
    "            \n",
    "        return all_detections"
   ],
   "metadata": {
    "id": "detector_class"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class KalmanFilter:\n",
    "    \"\"\"A simple Kalman Filter for tracking object motion.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.kf = cv2.KalmanFilter(7, 4)\n",
    "        self.kf.measurementMatrix = np.array([[1,0,0,0,0,0,0], [0,1,0,0,0,0,0], [0,0,1,0,0,0,0], [0,0,0,1,0,0,0]], np.float32)\n",
    "        self.kf.transitionMatrix = np.array([[1,0,0,0,1,0,0], [0,1,0,0,0,1,0], [0,0,1,0,0,0,0], [0,0,0,1,0,0,1], [0,0,0,0,1,0,0], [0,0,0,0,0,1,0], [0,0,0,0,0,0,1]], np.float32)\n",
    "        cv2.setIdentity(self.kf.processNoiseCov, 1e-2)\n",
    "        cv2.setIdentity(self.kf.measurementNoiseCov, 1e-1)\n",
    "        cv2.setIdentity(self.kf.errorCovPost, 1)\n",
    "\n",
    "    def predict(self) -> np.ndarray:\n",
    "        return self.kf.predict()\n",
    "\n",
    "    def update(self, bbox: List[float]):\n",
    "        x, y, w, h = bbox\n",
    "        measurement = np.array([x + w / 2, y + h / 2, w / h, h], dtype=np.float32).reshape(4, 1)\n",
    "        self.kf.correct(measurement)\n",
    "\n",
    "    def init(self, bbox: List[float]):\n",
    "        x, y, w, h = bbox\n",
    "        self.kf.statePost = np.array([x + w / 2, y + h / 2, w / h, h, 0, 0, 0], dtype=np.float32).reshape(7, 1)\n",
    "\n",
    "class STrack:\n",
    "    \"\"\"Represents a single tracked object.\"\"\"\n",
    "    def __init__(self, tlwh: List[float], score: float):\n",
    "        self.tlwh = np.asarray(tlwh, dtype=float)\n",
    "        self.score = score\n",
    "        self.kalman_filter = KalmanFilter()\n",
    "        self.kalman_filter.init(self.tlwh)\n",
    "        self.track_id = 0\n",
    "        self.state = 'new'\n",
    "        self.is_activated = False\n",
    "        self.frame_id = 0\n",
    "        self.start_frame = 0\n",
    "        self.time_since_update = 0\n",
    "\n",
    "    def activate(self, frame_id: int, track_id: int):\n",
    "        self.track_id = track_id\n",
    "        self.frame_id = frame_id\n",
    "        self.start_frame = frame_id\n",
    "        self.state = 'tracked'\n",
    "        self.is_activated = True\n",
    "\n",
    "    def re_activate(self, new_track, frame_id: int):\n",
    "        self.tlwh = new_track.tlwh\n",
    "        self.score = new_track.score\n",
    "        self.kalman_filter.update(self.tlwh)\n",
    "        self.state = 'tracked'\n",
    "        self.is_activated = True\n",
    "        self.frame_id = frame_id\n",
    "        self.time_since_update = 0\n",
    "\n",
    "    def predict(self):\n",
    "        if self.state != 'tracked':\n",
    "            self.kalman_filter.kf.statePost[6,0] = 0\n",
    "        self.kalman_filter.predict()\n",
    "\n",
    "    def update(self, new_track, frame_id: int):\n",
    "        self.tlwh = new_track.tlwh\n",
    "        self.score = new_track.score\n",
    "        self.kalman_filter.update(self.tlwh)\n",
    "        self.state = 'tracked'\n",
    "        self.is_activated = True\n",
    "        self.frame_id = frame_id\n",
    "        self.time_since_update = 0\n",
    "\n",
    "    @property\n",
    "    def tlbr(self) -> List[float]:\n",
    "        x, y, w, h = self.tlwh\n",
    "        return [x, y, x + w, y + h]\n",
    "\n",
    "def iou_distance(atracks: List[STrack], btracks: List[STrack]) -> np.ndarray:\n",
    "    if not atracks or not btracks: return np.empty((0, 0))\n",
    "    atlbrs = np.array([track.tlbr for track in atracks])\n",
    "    btlbrs = np.array([track.tlbr for track in btracks])\n",
    "    ious = np.zeros((len(atlbrs), len(btlbrs)))\n",
    "    for i, a in enumerate(atlbrs):\n",
    "        for j, b in enumerate(btlbrs):\n",
    "            box_inter = [max(a[0], b[0]), max(a[1], b[1]), min(a[2], b[2]), min(a[3], b[3])]\n",
    "            inter_area = max(0, box_inter[2] - box_inter[0]) * max(0, box_inter[3] - box_inter[1])\n",
    "            union_area = (a[2] - a[0]) * (a[3] - a[1]) + (b[2] - b[0]) * (b[3] - b[1]) - inter_area\n",
    "            if union_area > 0: ious[i, j] = inter_area / union_area\n",
    "    return 1 - ious\n",
    "\n",
    "class ByteTrack:\n",
    "    \"\"\"The main ByteTrack algorithm implementation.\"\"\"\n",
    "    def __init__(self, high_thresh: float = 0.6, low_thresh: float = 0.1, max_time_lost: int = 30):\n",
    "        self.tracked_stracks: List[STrack] = []\n",
    "        self.lost_stracks: List[STrack] = []\n",
    "        self.removed_stracks: List[STrack] = []\n",
    "        self.frame_id = 0\n",
    "        self.track_id_count = 0\n",
    "        self.high_thresh = high_thresh\n",
    "        self.low_thresh = low_thresh\n",
    "        self.max_time_lost = max_time_lost\n",
    "\n",
    "    def update(self, detections: List[Dict]) -> List[Dict]:\n",
    "        self.frame_id += 1\n",
    "        activated_starcks, refind_stracks, lost_stracks, removed_stracks = [], [], [], []\n",
    "        dets_high = [d for d in detections if d['confidence'] >= self.high_thresh]\n",
    "        dets_low = [d for d in detections if self.low_thresh <= d['confidence'] < self.high_thresh]\n",
    "        stracks_high = [STrack([*d['bbox'][:2], d['bbox'][2]-d['bbox'][0], d['bbox'][3]-d['bbox'][1]], d['confidence']) for d in dets_high]\n",
    "        stracks_low = [STrack([*d['bbox'][:2], d['bbox'][2]-d['bbox'][0], d['bbox'][3]-d['bbox'][1]], d['confidence']) for d in dets_low]\n",
    "\n",
    "        for strack in self.tracked_stracks: strack.predict()\n",
    "        dists = iou_distance(self.tracked_stracks, stracks_high)\n",
    "        matches, u_track, u_detection = self.linear_assignment(dists, 0.8)\n",
    "        for i, j in matches:\n",
    "            track = self.tracked_stracks[i]\n",
    "            det = stracks_high[j]\n",
    "            track.update(det, self.frame_id)\n",
    "            activated_starcks.append(track)\n",
    "\n",
    "        unmatched_tracks = [self.tracked_stracks[i] for i in u_track]\n",
    "        dists = iou_distance(unmatched_tracks, stracks_low)\n",
    "        matches, u_track, u_detection_low = self.linear_assignment(dists, 0.5)\n",
    "        for i, j in matches:\n",
    "            track = unmatched_tracks[i]\n",
    "            det = stracks_low[j]\n",
    "            track.update(det, self.frame_id)\n",
    "            activated_starcks.append(track)\n",
    "\n",
    "        for i in u_track:\n",
    "            track = unmatched_tracks[i]\n",
    "            track.state = 'lost'\n",
    "            lost_stracks.append(track)\n",
    "\n",
    "        for i in u_detection:\n",
    "            track = stracks_high[i]\n",
    "            if track.score >= self.high_thresh:\n",
    "                self.track_id_count += 1\n",
    "                track.activate(self.frame_id, self.track_id_count)\n",
    "                activated_starcks.append(track)\n",
    "\n",
    "        self.tracked_stracks = [t for t in self.tracked_stracks if t.state == 'tracked'] + activated_starcks\n",
    "        self.lost_stracks = [t for t in self.lost_stracks if t.time_since_update <= self.max_time_lost] + lost_stracks\n",
    "\n",
    "        output = [{'track_id': t.track_id, 'bbox': [int(x) for x in t.tlbr]} for t in self.tracked_stracks if t.is_activated]\n",
    "        return output\n",
    "\n",
    "    def linear_assignment(self, cost_matrix, thresh):\n",
    "        if cost_matrix.size == 0: return [], [], []\n",
    "        row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "        matches = [(r, c) for r, c in zip(row_ind, col_ind) if cost_matrix[r, c] < thresh]\n",
    "        u_track = [r for r in range(cost_matrix.shape[0]) if r not in [m[0] for m in matches]]\n",
    "        u_detection = [c for c in range(cost_matrix.shape[1]) if c not in [m[1] for m in matches]]\n",
    "        return matches, u_track, u_detection"
   ],
   "metadata": {
    "id": "bytetrack_class"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class PlayerFeatureExtractor(nn.Module):\n",
    "    \"\"\"A simple CNN to extract deep features from player patches.\"\"\"\n",
    "    def __init__(self, embedding_dim=128):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 512)\n",
    "        self.fc2 = nn.Linear(512, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 64 * 8 * 8)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.normalize(x, p=2, dim=1)\n",
    "\n",
    "class PlayerReID:\n",
    "    \"\"\"Manages long-term player identities using a hybrid feature model.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.feature_extractor = PlayerFeatureExtractor().to(self.device).eval()\n",
    "        self.ocr = easyocr.Reader(['en'], gpu=torch.cuda.is_available())\n",
    "        self.global_players: Dict[int, Dict] = {}\n",
    "        self.next_permanent_id = 1\n",
    "        self.similarity_threshold = 0.5\n",
    "        self.jersey_bonus = 0.4\n",
    "\n",
    "    def get_features(self, patch: np.ndarray) -> Dict:\n",
    "        \"\"\"Extracts color, deep, and jersey features from a player patch.\"\"\"\n",
    "        hsv = cv2.cvtColor(cv2.resize(patch, (64, 64)), cv2.COLOR_BGR2HSV)\n",
    "        color_hist = cv2.normalize(cv2.calcHist([hsv], [0, 1], None, [30, 32], [0, 180, 0, 256]), None).flatten()\n",
    "        img_tensor = torch.from_numpy(cv2.resize(patch, (64, 64))).permute(2, 0, 1).float().div(255).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            deep_features = self.feature_extractor(img_tensor).cpu().numpy().flatten()\n",
    "        jersey = None\n",
    "        try:\n",
    "            gray = cv2.cvtColor(patch, cv2.COLOR_BGR2GRAY)\n",
    "            results = self.ocr.readtext(gray, allowlist='0123456789', detail=0, paragraph=False)\n",
    "            if results and results[0].isdigit() and 1 <= len(results[0]) <= 2:\n",
    "                jersey = int(results[0])\n",
    "        except Exception:\n",
    "            pass\n",
    "        return {'color': color_hist, 'deep': deep_features, 'jersey': jersey}\n",
    "\n",
    "    def process_tracklets(self, tracklets_path: str, video_path: str, output_path: str) -> List[Dict]:\n",
    "        with open(tracklets_path, 'r') as f: all_tracklets = json.load(f)\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        long_tracks = []\n",
    "        \n",
    "        for frame_data in tqdm(all_tracklets, desc=\"Stage 3: Re-identifying players\"):\n",
    "            frame_id = frame_data['frame_id']\n",
    "            ret, frame = cap.read()\n",
    "            if not ret: break\n",
    "            \n",
    "            frame_tracks = {\"frame_id\": frame_id, \"players\": []}\n",
    "            for track in frame_data['tracks']:\n",
    "                x1, y1, x2, y2 = track['bbox']\n",
    "                patch = frame[y1:y2, x1:x2]\n",
    "                if patch.size == 0: continue\n",
    "                \n",
    "                current_features = self.get_features(patch)\n",
    "                best_id, best_score = None, self.similarity_threshold\n",
    "                \n",
    "                for pid, p_info in self.global_players.items():\n",
    "                    color_sim = cv2.compareHist(current_features['color'], p_info['features']['color'], cv2.HISTCMP_CORREL)\n",
    "                    deep_sim = cosine_similarity(current_features['deep'].reshape(1, -1), p_info['features']['deep'].reshape(1, -1))[0][0]\n",
    "                    sim = 0.4 * color_sim + 0.6 * deep_sim\n",
    "                    if current_features['jersey'] is not None and current_features['jersey'] == p_info['features']['jersey']:\n",
    "                        sim += self.jersey_bonus\n",
    "                    if sim > best_score:\n",
    "                        best_score, best_id = sim, pid\n",
    "                \n",
    "                if best_id is None:\n",
    "                    best_id = self.next_permanent_id\n",
    "                    self.next_permanent_id += 1\n",
    "                \n",
    "                if best_id in self.global_players:\n",
    "                    alpha = 0.1\n",
    "                    self.global_players[best_id]['features']['color'] = (1-alpha) * self.global_players[best_id]['features']['color'] + alpha * current_features['color']\n",
    "                    self.global_players[best_id]['features']['deep'] = (1-alpha) * self.global_players[best_id]['features']['deep'] + alpha * current_features['deep']\n",
    "                else:\n",
    "                    self.global_players[best_id] = {'features': current_features}\n",
    "                \n",
    "                frame_tracks[\"players\"].append({\"permanent_id\": best_id, \"bbox\": track['bbox'], \"jersey\": current_features['jersey']})\n",
    "            long_tracks.append(frame_tracks)\n",
    "        \n",
    "        cap.release()\n",
    "        with open(output_path, 'w') as f: json.dump(long_tracks, f, indent=2)\n",
    "        return long_tracks"
   ],
   "metadata": {
    "id": "reid_class"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class AdvancedEventDetector:\n",
    "    \"\"\"\n",
    "    Detects a wider range of soccer events by analyzing player kinematics,\n",
    "    interactions, and posture changes over time.\n",
    "    \"\"\"\n",
    "    def __init__(self, video_width: int, video_height: int, fps: float):\n",
    "        self.video_width = video_width\n",
    "        self.video_height = video_height\n",
    "        self.fps = fps if fps > 0 else 30.0\n",
    "        self.player_history = {}\n",
    "\n",
    "        self.sprint_velocity_threshold = 20.0\n",
    "        self.tackle_proximity_threshold = 75 \n",
    "        self.fall_aspect_ratio_threshold = 1.4\n",
    "        self.dribble_direction_change_threshold = 45\n",
    "        self.goal_area = [video_width * 0.8, 0, video_width, video_height]\n",
    "        self.celebration_cluster_size = 3\n",
    "        self.celebration_cluster_radius = 150\n",
    "        self.shot_to_celebration_window = 5 * self.fps\n",
    "\n",
    "    def _update_player_history(self, players, frame_id):\n",
    "        for p in players:\n",
    "            pid = p['permanent_id']\n",
    "            x1, y1, x2, y2 = p['bbox']\n",
    "            center = np.array([(x1 + x2) / 2, (y1 + y2) / 2])\n",
    "            width = x2 - x1\n",
    "            height = y2 - y1\n",
    "            aspect_ratio = width / height if height > 0 else 1\n",
    "\n",
    "            if pid not in self.player_history:\n",
    "                self.player_history[pid] = []\n",
    "            \n",
    "            history = self.player_history[pid]\n",
    "            velocity = np.array([0, 0])\n",
    "            speed = 0\n",
    "            if len(history) > 0:\n",
    "                prev_center = history[-1]['center']\n",
    "                velocity = center - prev_center\n",
    "                speed = np.linalg.norm(velocity)\n",
    "\n",
    "            history.append({\n",
    "                'frame_id': frame_id,\n",
    "                'center': center,\n",
    "                'bbox': p['bbox'],\n",
    "                'velocity': velocity,\n",
    "                'speed': speed,\n",
    "                'aspect_ratio': aspect_ratio\n",
    "            })\n",
    "            \n",
    "            if len(history) > self.fps * 2:\n",
    "                self.player_history[pid] = history[-int(self.fps * 2):]\n",
    "\n",
    "    def detect_events(self, player_tracks_path: str, output_path: str) -> List[Dict]:\n",
    "        with open(player_tracks_path, 'r') as f:\n",
    "            all_tracks = json.load(f)\n",
    "        \n",
    "        events = []\n",
    "        last_goal_area_entry = {}\n",
    "\n",
    "        for frame_data in tqdm(all_tracks, desc=\"Stage 4: Detecting events\"):\n",
    "            frame_id = frame_data['frame_id']\n",
    "            current_players = frame_data['players']\n",
    "            \n",
    "            self._update_player_history(current_players, frame_id)\n",
    "\n",
    "            player_map = {p['permanent_id']: p for p in current_players}\n",
    "            player_ids = list(player_map.keys())\n",
    "\n",
    "            for pid in player_ids:\n",
    "                history = self.player_history.get(pid, [])\n",
    "                if not history: continue\n",
    "                \n",
    "                if history[-1]['speed'] > self.sprint_velocity_threshold:\n",
    "                    events.append(self._create_event(frame_id, 'sprint', pid))\n",
    "                \n",
    "                if len(history) > 5:\n",
    "                    v1 = history[-5]['velocity']\n",
    "                    v2 = history[-1]['velocity']\n",
    "                    angle = self._angle_between(v1, v2)\n",
    "                    if angle > self.dribble_direction_change_threshold and history[-1]['speed'] > 5.0:\n",
    "                        events.append(self._create_event(frame_id, 'skill_move', pid))\n",
    "                \n",
    "                center = history[-1]['center']\n",
    "                if self.goal_area[0] < center[0] < self.goal_area[2] and self.goal_area[1] < center[1] < self.goal_area[3]:\n",
    "                    last_goal_area_entry[pid] = frame_id\n",
    "\n",
    "            for i in range(len(player_ids)):\n",
    "                for j in range(i + 1, len(player_ids)):\n",
    "                    pid1, pid2 = player_ids[i], player_ids[j]\n",
    "                    hist1, hist2 = self.player_history.get(pid1), self.player_history.get(pid2)\n",
    "\n",
    "                    if hist1 and hist2:\n",
    "                        dist = np.linalg.norm(hist1[-1]['center'] - hist2[-1]['center'])\n",
    "                        if dist < self.tackle_proximity_threshold:\n",
    "                            if hist1[-1]['aspect_ratio'] > self.fall_aspect_ratio_threshold:\n",
    "                                events.append(self._create_event(frame_id, 'tackle_fall', pid1))\n",
    "                            if hist2[-1]['aspect_ratio'] > self.fall_aspect_ratio_threshold:\n",
    "                                events.append(self._create_event(frame_id, 'tackle_fall', pid2))\n",
    "\n",
    "            player_centers = [p_hist[-1]['center'] for pid, p_hist in self.player_history.items() if p_hist and p_hist[-1]['frame_id'] == frame_id]\n",
    "            if len(player_centers) >= self.celebration_cluster_size:\n",
    "                from sklearn.cluster import DBSCAN\n",
    "                clustering = DBSCAN(eps=self.celebration_cluster_radius, min_samples=self.celebration_cluster_size).fit(player_centers)\n",
    "                if len(set(clustering.labels_)) > 1:\n",
    "                    for pid, entry_frame in last_goal_area_entry.items():\n",
    "                        if frame_id - entry_frame < self.shot_to_celebration_window:\n",
    "                            events.append(self._create_event(frame_id, 'goal_shot_attempt', pid))\n",
    "                            last_goal_area_entry = {}\n",
    "                            break\n",
    "\n",
    "        unique_events = self._deduplicate_events(events)\n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(unique_events, f, indent=2)\n",
    "        return unique_events\n",
    "\n",
    "    def _create_event(self, frame_id, event_type, player_id=None):\n",
    "        event = {\n",
    "            'frame_id': frame_id,\n",
    "            'timestamp': frame_id / self.fps,\n",
    "            'event_type': event_type,\n",
    "        }\n",
    "        if player_id:\n",
    "            event['player_id'] = player_id\n",
    "        return event\n",
    "    \n",
    "    def _angle_between(self, v1, v2):\n",
    "        v1_u = v1 / np.linalg.norm(v1) if np.linalg.norm(v1) > 0 else np.array([0,0])\n",
    "        v2_u = v2 / np.linalg.norm(v2) if np.linalg.norm(v2) > 0 else np.array([0,0])\n",
    "        rad = np.arccos(np.clip(np.dot(v1_u, v2_u), -1.0, 1.0))\n",
    "        return np.rad2deg(rad)\n",
    "\n",
    "    def _deduplicate_events(self, events):\n",
    "        if not events:\n",
    "            return []\n",
    "        \n",
    "        sorted_events = sorted(events, key=lambda x: x['frame_id'])\n",
    "        \n",
    "        unique_events = [sorted_events[0]]\n",
    "        for current_event in sorted_events[1:]:\n",
    "            last_event = unique_events[-1]\n",
    "            if (current_event['frame_id'] - last_event['frame_id']) < self.fps * 2:\n",
    "                continue\n",
    "            unique_events.append(current_event)\n",
    "            \n",
    "        return unique_events"
   ],
   "metadata": {
    "id": "advanced_event_detector_class"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class VideoAssembler:\n",
    "    \"\"\"Assembles the final highlight reel using intelligent scene cutting.\"\"\"\n",
    "    def __init__(self, clip_padding_seconds: float = 1.0, max_reel_duration: float = 300.0):\n",
    "        self.clip_padding = clip_padding_seconds\n",
    "        self.max_duration = max_reel_duration\n",
    "        self.temp_dir = '/content/temp_clips'\n",
    "\n",
    "    def find_scenes(self, video_path: str) -> List[Tuple[float, float]]:\n",
    "        \"\"\"Use PySceneDetect to find all scene boundaries in the video.\"\"\"\n",
    "        video = open_video(video_path)\n",
    "        scene_manager = SceneManager()\n",
    "        scene_manager.add_detector(ContentDetector(threshold=27.0))\n",
    "        scene_manager.detect_scenes(video, show_progress=False)\n",
    "        scene_list = scene_manager.get_scene_list()\n",
    "        return [(s[0].get_seconds(), s[1].get_seconds()) for s in scene_list]\n",
    "\n",
    "    def assemble_highlight_reel(self, video_path: str, player_events_path: str, output_path: str, target_player_id: int) -> bool:\n",
    "        with open(player_events_path, 'r') as f: all_events = json.load(f)\n",
    "        \n",
    "        player_events = [e for e in all_events if e.get('player_id') == target_player_id or 'cluster' in e['event_type'] or 'goal' in e['event_type']]\n",
    "        \n",
    "        if not player_events: \n",
    "            print(\"No events for target player. Cannot create reel.\")\n",
    "            return False, 0\n",
    "        \n",
    "        print(\"Finding scene cuts... (this may take a moment)\")\n",
    "        scenes = self.find_scenes(video_path)\n",
    "        event_timestamps = sorted([e['timestamp'] for e in player_events])\n",
    "        \n",
    "        clips_to_extract = []\n",
    "        total_duration = 0.0\n",
    "        for ts in event_timestamps:\n",
    "            if total_duration >= self.max_duration: break\n",
    "            for start, end in scenes:\n",
    "                if start <= ts <= end:\n",
    "                    clip_duration = end - start\n",
    "                    if total_duration + clip_duration <= self.max_duration:\n",
    "                        if not any(abs(c['start'] - start) < 1.0 for c in clips_to_extract):\n",
    "                            clips_to_extract.append({'start': start, 'end': end})\n",
    "                            total_duration += clip_duration\n",
    "                    break\n",
    "        \n",
    "        if not clips_to_extract: \n",
    "            print(\"Could not map any events to scenes.\")\n",
    "            return False, 0\n",
    "        \n",
    "        shutil.rmtree(self.temp_dir, ignore_errors=True)\n",
    "        os.makedirs(self.temp_dir, exist_ok=True)\n",
    "        clip_paths = []\n",
    "        for i, clip in enumerate(tqdm(clips_to_extract, desc=\"Extracting scene clips\")):\n",
    "            clip_path = os.path.join(self.temp_dir, f\"clip_{i:04d}.mp4\")\n",
    "            command = ['ffmpeg', '-y', '-i', video_path, '-ss', str(clip['start']), '-to', str(clip['end']), '-c', 'copy', '-avoid_negative_ts', '1', clip_path]\n",
    "            result = subprocess.run(command, capture_output=True, text=True)\n",
    "            if result.returncode == 0: clip_paths.append(clip_path)\n",
    "        \n",
    "        concat_list_path = os.path.join(self.temp_dir, \"concat_list.txt\")\n",
    "        with open(concat_list_path, 'w') as f:\n",
    "            for path in clip_paths:\n",
    "                f.write(f\"file '{os.path.basename(path)}'\\n\")\n",
    "        \n",
    "        concat_command = ['ffmpeg', '-y', '-f', 'concat', '-safe', '0', '-i', concat_list_path, '-c', 'copy', output_path]\n",
    "        concat_result = subprocess.run(concat_command, cwd=self.temp_dir, capture_output=True, text=True)\n",
    "\n",
    "        if concat_result.returncode == 0:\n",
    "            print(f\"Highlight reel created: {output_path}\")\n",
    "            print(f\"Total duration: {total_duration:.2f} seconds\")\n",
    "            return True, len(player_events)\n",
    "        else:\n",
    "            print(\"Failed to concatenate clips. FFmpeg error:\", concat_result.stderr)\n",
    "            return False, len(player_events)"
   ],
   "metadata": {
    "id": "assembler_class"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "if 'video_path' in locals() and video_path:\n",
    "    detections_path = '/content/output/detections.json'\n",
    "    tracklets_path = '/content/output/tracklets.json'\n",
    "    long_tracks_path = '/content/output/long_player_track.json'\n",
    "    events_path = '/content/output/player_events.json'\n",
    "    target_player_id = 1\n",
    "    highlight_path = f'/content/output/player_{target_player_id}_highlights.mp4'\n",
    "\n",
    "    detector = SoccerPlayerDetector()\n",
    "    detections = detector.process_video(video_path, detections_path)\n",
    "    print(f\"✅ Detection complete! Processed {len(detections)} frames.\")\n",
    "\n",
    "    tracker = ByteTrack()\n",
    "    all_tracklets = []\n",
    "    for frame_data in tqdm(detections, desc=\"Stage 2: Tracking players\"):\n",
    "        tracks = tracker.update(frame_data['detections'])\n",
    "        all_tracklets.append({\"frame_id\": frame_data['frame_id'], \"tracks\": tracks})\n",
    "    with open(tracklets_path, 'w') as f: json.dump(all_tracklets, f, indent=2)\n",
    "    print(f\"✅ Tracking complete! Processed {len(all_tracklets)} frames.\")\n",
    "\n",
    "    reid = PlayerReID()\n",
    "    long_tracks = reid.process_tracklets(tracklets_path, video_path, long_tracks_path)\n",
    "    print(f\"✅ Re-ID complete! Identified {len(reid.global_players)} unique players.\")\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    video_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    video_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    cap.release()\n",
    "\n",
    "    event_detector = AdvancedEventDetector(video_width, video_height, fps)\n",
    "    player_events = event_detector.detect_events(long_tracks_path, events_path)\n",
    "    print(f\"✅ Advanced event detection complete! Found {len(player_events)} unique highlight events.\")\n",
    "\n",
    "    assembler = VideoAssembler()\n",
    "    success, num_events = assembler.assemble_highlight_reel(video_path, events_path, highlight_path, target_player_id)\n",
    "    if success:\n",
    "        print(f\"✅ Highlight reel saved to: {highlight_path}\")\n",
    "    else:\n",
    "        print(\"❌ Failed to create highlight reel\")\n",
    "else:\n",
    "    print(\"❌ No video path available. Please run the upload cell first.\")"
   ],
   "metadata": {
    "id": "run_pipeline"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 📥 4. Download Results"
   ],
   "metadata": {
    "id": "download_markdown"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import files\n",
    "\n",
    "if 'success' in locals() and success and 'highlight_path' in locals() and os.path.exists(highlight_path):\n",
    "    files.download(highlight_path)\n",
    "    print(f\"✅ Downloaded: {os.path.basename(highlight_path)}\")\n",
    "else:\n",
    "    print(f\"❌ Could not download highlight reel. File not found or creation failed.\")\n",
    "\n",
    "if os.path.exists('/content/output/long_player_track.json'):\n",
    "    files.download('/content/output/long_player_track.json')\n",
    "    print(\"✅ Downloaded: long_player_track.json\")\n",
    "\n",
    "if os.path.exists('/content/output/player_events.json'):\n",
    "    files.download('/content/output/player_events.json')\n",
    "    print(\"✅ Downloaded: player_events.json\")\n",
    "\n",
    "print(\"\\n🎉 Pipeline complete! Check your browser's downloads folder.\")"
   ],
   "metadata": {
    "id": "download_results"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"📊 PIPELINE SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if 'video_path' in locals() and video_path and os.path.exists('/content/output/long_player_track.json') and os.path.exists('/content/output/player_events.json'):\n",
    "    with open('/content/output/long_player_track.json', 'r') as f:\n",
    "        long_tracks_summary = json.load(f)\n",
    "    with open('/content/output/player_events.json', 'r') as f:\n",
    "        player_events_summary = json.load(f)\n",
    "\n",
    "    print(f\"📹 Video processed: {os.path.basename(video_path)}\")\n",
    "    print(f\"🎯 Target player ID: {target_player_id}\")\n",
    "    print(f\"🖼️ Total frames processed: {len(long_tracks_summary)}\")\n",
    "    print(f\"✨ Events detected for player: {num_events}\")\n",
    "\n",
    "    if 'success' in locals() and success and 'highlight_path' in locals() and os.path.exists(highlight_path):\n",
    "        print(f\"⏱️ Highlight reel generated successfully.\")\n",
    "        file_size = os.path.getsize(highlight_path) / (1024 * 1024)\n",
    "        print(f\"📁 Output file size: {file_size:.1f} MB\")\n",
    "    else:\n",
    "        print(\"⏱️ Highlight reel was not generated.\")\n",
    "else:\n",
    "    print(\"Could not generate summary. One or more output files are missing.\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"🎉 Pipeline finished!\")"
   ],
   "metadata": {
    "id": "summary"
   },
   "execution_count": null,
   "outputs": []
  }
 ]